\chapter{Introdução e preliminares} 

% Talvez falte algo inicial sobre o tema de fato do trabalho, que é algoritmos com predições (estudo de caso: problema de caching). Talvez só o resumo mesmo.... 

% Montar esqueleto (ordenação) dos argumentos

% Análise de pior caso é pessimista, não rankea algoritmos corretamente na prática
% Em algoritmos online, qualquer métrica absoluta é inútil, então -> análise competitiva (que ainda olha o pior caso)
% Indo "para além do pior caso", modelar entrada ou prever entrada: algoritmos online com predições. Falar de FIFO vs LRU.
% Estudo de caso problema de caching

Os \textbf{algoritmos online} são algoritmos que recebem e processam a entrada, uma sequência de pedidos, em passos. Em um problema online, o algoritmo deve processar um pedido sem o conhecimento do próximo pedido. Em cada passo, uma decisão é tomada, de acordo com uma \emph{política}, dentre várias alternativas possíveis, cada uma com um custo associado e, portanto, a decisão tomada em um passo pode afetar os custos das alternativas de pedidos futuros. Em contrate, um algoritmo offline recebe todos os pedidos a priori, antes de começar a processar. 

Na construção de algoritmos online típicos, não é proveitoso ter em mente qualquer métrica absoluta de desempenho, como na análise tradicional de pior caso, em que é dada uma garantia forte contra qualquer entrada. De fato, mostra-se que, na maioria dos problemas, qualquer algoritmo que processa pedidos pode ser forçado a incorrer um custo ilimitado ao escolher cuidadosamente uma entrada patológica. Portanto, uma métrica absoluta é superficial ao comparar políticas que competem. Para contornar isso, em problemas online, é comum a análise comparativa chamada \textbf{análise competitiva} na qual o desempenho de um algoritmo é comparado com o desempenho do algoritmo offline ótimo para o mesmo problema. Um algoritmo é dito \emph{competitivo} se sua razão de competitividade ou simplesmente competitividade — a razão entre seu custo total e o custo total do algoritmo offline de acordo com uma determinada métrica — for limitada para qualquer entrada.

O objetivo principal da análise matématica de algoritmos é informar sobre os melhores algoritmos para resolver um determinado problema computacional. A análise tradicional de algoritmos online foca no pior caso, onde a competitividade de um algoritmo é determinada pelo seu pior desempenho sobre qualquer entrada, implicitamente classificando como o melhor algoritmo aquele que garante o melhor desempenho, ou seja, o menor custo, em entradas difíceis. No entanto, em muitos problemas fundamentais, essa robustez não é satisfatória. Por exemplo, no tradicional \textbf{problema de caching}, que será estudado neste trabalho, a pespectiva de pior caso não é suficiente para decidir a melhor política entre \emph{LRU} e \emph{FIFO}, apesar da evidente superiodade da primeira sobre a segunda na prática. Outro caso é no problema (offline) de ordenação de listas. Sabe-se que o algoritmo \textsc{MergeSort} leva tempo \(\Theta(n \log n)\) para ordenar uma lista de tamanho \(n\), mesmo que a entrada já esteja ordenada, enquanto o tempo do \textsc{InsertionSort} é \(\Theta(n)\) para listas ordenadas. Então, em um espaço de entradas de listas parcialmente ordenadas, o \textsc{InsertionSort} é mais eficiente, apesar de ter uma garantia apenas quadrática em geral. Conclusão: na comparação entre algoritmos, o desemepenho superior de um sobre o outro é determinado pelos dados de entrada e, então, a escolha é dependente do contexto. 

 Na análise tradicional de algortimos online, não há um modelo realista para os dados de entrada, e, então, é pessimista em contextos onde as entradas difíceis são relativamente raras. Diante dessa dificuldade intríseca na comparação de algoritmos, principalmente em problemas online, foi importante encontrar outras abordagens mais sofisticadas. Essa busca deu origem à filosofia \textbf{para além da análise de pior caso}, introduzida principalmente por \textcite{Roughgarden19}, que explora outros tipos de análise. Nesse trabalho, são propostos diferentes modelos que buscam ser mais realistas em cada contexto. O resultado são garantias mais otimistas para os algoritmos que explicam a superioridade observada na prática e auxiliam na construção de novos algoritmos.

 Este trabalho foca no \textbf{problema de caching} como estudo de caso para análise competitiva e algoritmos online. No problema de caching, é mantido um \emph{cache} de \(k\) \emph{páginas} enquanto se processa uma sequência de pedidos a páginas, que podem ou não estar no cache. De forma simplificada, assuma que as páginas são sempre servidas no cache e que trazer uma página para o cache tem custo unitário (em particular, se o cache estiver cheio, trazer uma página ao cache requer que outra seja descartada). O objetivo é minimar o número de \emph{falhas de cache}, isto é, pedidos nos quais a página solicitada não já está no cache. 

 Na pespectitiva tradicional de pior caso, o problema de caching foi amplamente estudado, como em \textcite{Achlioptas00}. Nessa pespectitiva, não é possível capturar a superioridade da política \emph{LRU} (que sempre descarta a página com o pedido mais antigo, numa falha de cache) e \emph{FIFO} (que descarta a página mais antiga a entrar no cache) observada na prática. Para resolver essa questão e outras similares foram propostas várias soluções. A ideia de fazer suposições sobre a estrutura dos dados de entrada, por exemplo, é comum na literatura; em \textcite{Albers02}, é proposto um modelo de entradas, supostamente mais realista, baseado na \emph{localidade de referência} (...).

 Aqui, seguimos uma direção alternativa e não supomos nada sobre a estrutura dos dados de entrada. Os \textbf{algoritmos com predições}, introduzidas por \cite{Lykouris18}, são um framework "para além do pior caso" que utilizam a crescente capacidade dos modelos de aprendizado para melhorar e analisar os algoritmos. Nesse framework, os algoritmos onlines tradicionais são aprimorados com ajuda de um \emph{oráclo} que prevê entradas futuras. Esse oráclo, tratado como caixa-preta, classifica implicitamente as entradas como "fáceis" ou "difíceis" de acordo com sua previsibilidade. As instâncias fáceis são aquelas que o oráclo, construído com a última tecnologia de aprendizado, seja perceptrons, redes neurais, SVM's, ou qualquer modelo que venha no futuro, tem pouco erro; as "difíceis" são aquelas em que a qualidade de predição é baixa. 

 A dificuldade de confiar cegamente no oráclo é que os modelos de aprendizado geralmente tem poucas garantias de pior caso. Por isso, nos algoritmo com predições, buscamos duas coisas, que capturam o "melhor dos dois mundos". i) queremos que o algoritmo seja \emph{consistente}, isto é, desempenhe melhor quanto melhor for a qualidade da predição e, no caso limite de um oráclo sem erros, idealmente tenha o desempenho do algoritmo offline ótimo ii) que seja \emph{robusto}: não deve desempenhar pior que os algoritmos tradicionais que não possuem oráclo. Nesse sentido, busca-se estudar o \emph{trade-off} entre competitividade e erro do oráclo.

Então, com os algoritmos aumentados, buscamos obter o melhor dos dois mundos: um algoritmo que desempenha bem no caso otimista, onde a predição tem erro baixo e se mantém robusto no sentido do pior caso tradicional, onde a predição pode ser arbitrariamente ruim. 

% Marker, resultados quantitativos de competitividade

\textbf{O objetivo deste trabalho é fornecer a estudantes e pesquisadores um primeiro passo na literatura recente de algortimos de predições. Com enfoque no problema clássico de caching, apresentamos os conceitos teóricos base e discutimos os principais resultados e algoritmos.}

\section{Notação e preliminares}

Nessa seção coletamos as notações básicas que serão usadas ao longo do texto. O leitor pode dispensar essa seção e usar como referência quando necessário.

Ao longo do texto, usaremos \textbf{negrito} para definições formais e \textit{itálico} para definições informais ou simplesmente para ênfase.

\subsection{Sequências}
\label{sec:seq}

Para qualquer conjunto não vazio \(X\), definimos \(\Seq(X) \coloneqq \bigcup_{n = 0}^\infty X^n\) e definimos as \textbf{sequências} como sendo os elementos de~\(\Seq(X)\). Seja \(T \in \Naturals\) e seja \(X\) um conjunto não vazio. Ao longo do texto, manteremos a convenção de usar letras em negrito para sequências e, para denotar os elementos da sequência, usamos a letra sem negrito com um índice. Formalmente, para qualquer \(\boldx \in X^T \subseteq \Seq(X)\) (ou seja, uma sequência de elementos de \(X\) de tamanho \(T\): \(|\boldx| = T)\), definimos \(x_i \coloneqq \boldx_i\) para cada \(i \in [T]\). Além disso, usamos colchetes angulares para escrever sequências por extenso, ou seja, para todo \(\boldx \in X^T\) e \(i,j \in [T]\) com \(i \leq j\), temos que \(\angb{x_i, \dotsc, x_j}\) denota a sequência cujos \(j - i + 1\) elementos são, em ordem\footnote{Note   que, pela definição de sequência, os índices dos elementos de   \(\angb{x_i, \dotsc, x_j}\) variam de \(1\) a \(j - i +   1\). Portanto, para qualquer \(k \in \iinterval{1}{j - i + 1}\), o   \(k\)-ésimo elemento de \(\angb{x_i, \dotsc, x_j}\) é   \(x_{i + k - 1}\). }, \(x_i, x_{i+1}, \dotsc, x_{j}\). Finalmente,~\(\angb{}\) denota a sequência vazia, e para qualquer \(\boldx \in X^T\) e \(i,j \in \Integers\) tal que \(j < i\) ou \(j < 0\), definimos \(\angb{x_i, \dotsc, x_j} \coloneqq \angb{}\). Nós também denotamos por \(\boldx_{i:j}\) a sequência \(\angb{x_i, \dotsc, x_j}\) para qualquer \(\boldx \in \Seq(X)\) e~\(i,j \in \Integers\). Escrevemos que um elemento \(a \in \boldx\) se existe \(i \in [n]\) tal que \(a = x_i\).

Será útil introduzir mais uma noção para sequências. Sejam \(\boldx \in X^T \subseteq \Seq(X)\) e \(\boldl \in \Naturals^m \subseteq \Seq(\Naturals)\) uma sequência de índices de tamanho \(m\) tal que \(1 = l_1 \le l_2 \le \dotsc \le l_{m-1} \le l_{m} = T\). Definimos a \textbf{partição contígua} de \(\boldx\) \textbf{determinada} por \(\boldl\) como a sequência de \textbf{blocos contíguos} \(\boldb \in \Seq(X)^m\) em que \(b_i = x_{l_i:l_{i+1}-1}\), \(i \in [1,m-1]\) e \(b_m = x_{l_m:T}\). Intuitivamente, \(\boldl\) indica os índices de início de cada bloco e \(\boldb\) é o conjunto de blocos consecutivos resultantes. A concatenação de todos os blocos reconstrói a sequência original. Assim, se \(\boldx = \angb{x_1, x_2, x_3, x_4, x_5, x_6}\) e \(\boldl = \angb{1,4,5}\) A partição contígua de \(\boldx\) determinada por \(\boldl\) é \(\boldb = \angb{\angb{x_1, x_2, x_3}, \angb{x_4, x_5}, \angb{x_6}}\). A maior parte da notação para sequências está listada na \Cref{tbl:seq_notation}.

\bgroup
	\renewcommand{\arraystretch}{1.2}
	\begin{table}[htbp]
	  \caption{Notação básica}
          \label{tbl:notation}
	  \centering
	  \begin{tabular}{r c p{13cm}}
	    \toprule
            \(\ind{P}\) & \(\coloneqq\)& \(1\) se o predicado \(P\) for verdadeiro, e zero caso contrário \\
            \(\binom{X}{n}\) & \(\coloneqq\)& o conjunto de todos os subconjuntos de \(X\) de tamanho \(n\) para cada \(n \in [|X|]\) \\
            \(X^Y\) & \(\coloneqq\)& o conjunto das funções com domínio \(Y\)
                                     e contra domínio \(X\)\\
            \(X + y\) & \(\coloneqq\)& \(X \cup \{y\}\) dado que \(X\subseteq U\) e \(y \in U\) \\
            \(X - y\) & \(\coloneqq\)& \(X \setminus \{y\}\) dado que \(X\subseteq U\) e \(y \in U\)\\
            \bottomrule
	  \end{tabular}
	\end{table}
\egroup

\bgroup
	\renewcommand{\arraystretch}{1.2}
	\begin{table}[htbp]
	  \caption{Conjuntos frequentemente usados}
          \label{tbl:known_setsnotation}
	  \centering
	  \begin{tabular}{r c p{13cm}}
	    \toprule
            \([n]\)
            & \(\coloneqq\)
            & \(\{1, \dotsc, n\}\) para cada \(n
              \in \Naturals\)\\
              \(\mathbb{N}_0\)
            & \(\coloneqq\)
            & \(\Naturals \cup \{0\}\) \\
            \bottomrule
	  \end{tabular}
	\end{table}
\egroup
%
\bgroup
	\renewcommand{\arraystretch}{1.3}
	\begin{table}[htbp]
	  \caption{Notação para sequências}
          \label{tbl:seq_notation}
	  \centering
	  \begin{tabular}{r c p{13cm}}
	    \toprule
            \(X^*\)
            &\(\coloneqq\)
            & \(\bigcup_{n = 0}^\infty X^n\)\\
            \(|\boldx|\)
            & \(\coloneqq\)
            & o tamanho da sequência \(\boldx\)\\
            \(\emptyseq\)
            & \(\coloneqq\)
            & a sequência vazia\\
            \(\angb{x_1, \dotsc, x_t}\)
            & \(\coloneqq\)
            & a sequência de tamanho \(t\) em que o \(k\)-th
              elemento é \(x_k\) para cada \(k \in [t]\) \\
            \(\angb{x_i, \dotsc, x_j}\)
            & \(\coloneqq\)
            & \(\emptyseq\) se \(i > j\), a sequência em que o \(k\)-th elemento
              é \(x_{i + k -1}\) para cada \(k \in [j - i + 1]\)
              caso contrário\\
            \(\boldx_{i:j}\)
            & \(\coloneqq\)
            & \(\angb{x_i, \dotsc, x_j}\)\\
            \(x[i:j]\)
            & \(\coloneqq\)
            & \(\boldx_{i:j}\) \\
            \bottomrule
	  \end{tabular}
	\end{table}
\egroup


\subsection{Probabilidade}

Sejam \(X\) e \(Y\) variáveis aleatórias quaisquer. Temos que,

\begin{equation}
  \label{eq:linearidade}
  \Expect[X + Y] = \Expect[X] + \Expect[Y]
\end{equation}

Se \(X\) e \(Y\) são independentes, então,
\begin{equation}
  \label{eq:inde}
  \Expect[X.Y] = \Expect[X]\Expect[Y]
\end{equation}

Para uma variável aleatória discreta \( X \ge 0 \), vale a identidade
\begin{equation}
  \label{eq:va_positiva}
\Expect[X] = \sum_{k=1}^{\infty} \Pr(X \ge k),
\end{equation}

Sejam \(\Xcal\) universo finito e não vazio e \(X\) uma variável aleatória tal que \(\forall x \in \Xcal \ , Pr(X = x) = \frac{1}{|S|}\) . Dizemos, então, que \(X\) tem \textbf{distribuição uniforme sobre} \(Y\) e denotamos isso por \(X \sim \mathrm{Unif}(Y)\).

Seja \(\boldX\) uma sequência de variáveis aleatórias independentes, tais que $\Pr(X_i = 1) = p$ e $\Pr(X_i = 0) = 1 - p$, com $p \in (0,1)$. Defina \(T = \min \{ i \in \Naturals : X_i = 1 \}\). Dizemos, então, que $T$ tem \textbf{distribuição geométrica} com parâmetro $p$ e denotamo isso com \(T \sim \mathrm{Geom}(p)\). A função de probabilidade de $T$ é dada por \(\Pr(T = k) = (1 - p)^{k - 1} p\) \(\quad k = 1, 2, \dots\).

\begin{proposition}
\label{prop:geom}
Seja \(X\) uma variável aleatória de distribuição geométrica de parâmetro \(p\), então \(\Expect[X] = 1/p\).
\begin{proof}
\begin{equation*}
\Expect[X] = \sum_{k=1}^{\infty} k \Pr(X = k) = p \sum_{k=1}^{\infty} k (1 - p)^{k - 1}.
\end{equation*}
Defina \(r \coloneq 1 - p\) e \(S \coloneq \sum_{k=1}^{\infty} k r^{k - 1}\). Então,
\begin{align*}
  S
  &=\sum_{k=1}^{\infty} k r^{k - 1} = \sum_{k=0}^{\infty} (k+1) r^{k} \\
  &=  r\sum_{k=0}^{\infty} kr^{k-1} + \sum_{k=0}^{\infty} r^{k} \\
  &= Sr + 1/(1-r).
\end{align*}

Daí, segue que, \(Sp = \Expect[T] = 1/p\).

\end{proof}
\end{proposition}

\begin{lemma}[Identidade de Wald]
  \label{lema:wald}
  Seja \(\boldX\) uma sequência de variáveis aleatórias reais, não negativas, independentes e identicamente distribuídas. Além disso, seja \(\tau\) uma variável aleatória tomando valores não negativos tal que o evento \(\{\tau \le n\}\) seja independente de \(\{X_i \colon i \ge n\}\) para qualquer \(n \ge 0\). Então,
  \begin{equation*}
    \Expect\left[\sum_{i=1}^\tau X_i \right ] = \Expect[\tau].\Expect[X_1]
  \end{equation*}
  \begin{proof}
    Temos que,
\begin{align*}
  \Expect \left[ \sum_{i=1}^{\tau} X_i \right]
  &= \Expect \left[ \sum_{i=1}^{\infty} X_i \ind{\tau \ge i} \right] \\
  \stackrel{\eqref{eq:linearidade}}{=}&
   \sum_{i=1}^{\infty} \Expect \left[ X_i \ind{\tau \ge i} \right] \\
  \stackrel{\eqref{eq:inde}}{=}&
   \Expect[X_1] \sum_{i=1}^{\infty} \Pr[\tau \ge i] \\
  \stackrel{\eqref{eq:va_positiva}}{=}&
   \Expect[X_1] \cdot \Expect[\tau]
\end{align*}
  \end{proof}
\end{lemma}

\begin{theorem}[Coletor de cupons]
  \label{teo:cupon}
  Sejam \(X\) um universo abstrato de \(n\) elementos e \(\boldZ\) uma sequência de variáveis aleatórias independentes tal que \(X_i\) é tomado uniformemente em \(U\). Defina,
  \begin{equation*}
    L = \inf\{T \colon x \in \boldZ_{1:T} \quad \forall x \in X\}.
  \end{equation*}
  Então, temos que, \(\Expect[L] = nH_n\), onde \(H_i = \sum_{j=1}^i 1/j\) é o i-ésimo número harmônico. Esse resultado é conhecido como \emph{teorema do colecionador de cupons} e se refere ao seguinte problema abstrato: se cada caixa de uma marca de cereais contém um cupom e existem \(n\) tipos diferentes de cupons,  dados os \(n\) cupons, quantos cupons você espera que precise remover com substituição antes de remover cada um dos cupons pelo menos uma vez?

  \begin{proof}
    Defina a sequência \(\boldl \in \Seq(\Naturals)^n\) tal que \(l_i\) é o tempo que decorreu entre o momento em que \(i-1\) cupons foram coletadas e \(i\) cupons foram coletados. A probabilidade \(p_i\) de um novo cupon ser coletado após \(i-1\) terem sidos coletados é \(\frac{n-i+1}{n}\). Logo, \(l_i \sim \mathrm{Geom}(p_i)\) e, pela \Cref{prop:geom} valor esperado \(1/p_i\). Como \(L = \sum_{i=1}^n l_i\), segue que,
    \begin{align*}
      \Expect[L] = \Expect\left[\sum_{i=1}^n l_i\right]
      \stackrel{\eqref{eq:linearidade}}{=}&
      \sum_{i=1}^n \Expect[l_i] = \sum_{i=1}^n \frac{n}{n-i+1} = nH_n
    \end{align*}
  \end{proof}

\end{theorem}

\begin{theorem}[Princípio de Minimax]
  \label{teo:minimax}
 Sejam \(X\) uma variável aleatória tomando valores em \(\Xcal\) e \(Y\) tomando valores em \(\Ycal\), então,
 \begin{equation*}
    \max_{x \in \Xcal} \Expect[c(\Acal,x)] = \min_{A \in \Acal} \Expect[c(a,X)]
  \end{equation*}
  \begin{proof}
\begin{proof}
Primeiro, escrevemos as esperanças como somas sobre todos os possíveis valores de \(X\) e \(A\):
\begin{equation*}
\Expect[c(A, x)] = \sum_{a \in \mathcal{A}} \Pr[A = a]\, c(a, x)
\quad \text{e} \quad
\Expect[c(a, X)] = \sum_{x \in \mathcal{X}} \Pr[X = x]\, c(a, x).
\end{equation*}

Sabemos que a média ponderada de uma sequência é sempre limitada superiormente pelo seu valor máximo.  
No nosso caso, os pesos são \(\Pr[X = x]\).  
Como \(\sum_{x \in \mathcal{X}} \Pr[X = x] = 1\), temos:
\begin{equation*}
\max_{x \in \mathcal{X}} \Expect[c(A, x)]
= \max_{x \in \mathcal{X}} \sum_{a \in \mathcal{A}} \Pr[A = a]\, c(a, x)
\ge \sum_{x \in \mathcal{X}} \Pr[X = x] \sum_{a \in \mathcal{A}} \Pr[A = a]\, c(a, x).
\end{equation*}

Podemos agora reordenar as somas e obter:
\begin{equation*}
\sum_{x \in \mathcal{X}} \Pr[X = x] \sum_{a \in \mathcal{A}} \Pr[A = a]\, c(a, x)
= \sum_{a \in \mathcal{A}} \Pr[A = a] \sum_{x \in \mathcal{X}} \Pr[X = x]\, c(a, x).
\end{equation*}

Finalmente, usando que \(\sum_{a \in \mathcal{A}} \Pr[A = a] = 1\), obtemos:
\begin{equation*}
\sum_{a \in \mathcal{A}} \Pr[A = a] \sum_{x \in \mathcal{X}} \Pr[X = x]\, c(a, x)
\ge \min_{a \in \mathcal{A}} \sum_{x \in \mathcal{X}} \Pr[X = x]\, c(a, x)
= \min_{a \in \mathcal{A}} \Expect[c(a, X)].
\end{equation*}
\end{proof}
  \end{proof}
\end{theorem}
