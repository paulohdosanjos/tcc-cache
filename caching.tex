% LTeX: language=pt-BR
\chapter{Problema de caching tradicional}

O problema de \emph{caching} considera um sistema com dois níveis de memória: uma memória rápida ou cache de tamanho \(k\) e uma memória lenta de tamanho \(m \gg k\). Uma sequência de itens (páginas), é fornecida como entrada ao \emph{algoritmo de caching} e deve ser processada em ordem. Se a página referenciada estiver no cache, o pedido é satisfeito com custo zero e dizemos que houve um \emph{acerto de cache}; caso contrário, acontece uma \emph{falha de cache}. Neste caso, o algoritmo traz a página da memória lenta e coloca no cache antes de satisfazer o pedido. Se o cache estiver cheio, uma página no cache deve ser descartada. O algoritmo, então, escolhe uma página a ser descartada de acordo com uma regra de substituição ou \emph{política}. O objetivo do problema é encontrar uma política que minimize a quantidade total de falhas de cache. Antes de entrarmos no modelos de predições, estudaremos o problema tradicional, ou seja, sem predições.

%resumo do capítulo também

\section{Formalização do problema}
Tendo como base a descrição informal dada acima, definimos formalmente o problema de caching. Como é de costume na literatura que trata desse problema sobre a pespectiva do custo de falhas de cache, não será necessária uma notação muito rigorosa e, por isso, definimos o problema a partir de uma notação simples e suficientemente clara para nossos propósitos.

\begin{definition}[Problema de caching]
  \label{def:cache}
  Uma \textbf{instância de caching} é definida pela tripla \((\Zcal, k, \pi) \), onde \(\Zcal\) é um conjunto arbitrário chamado de conjunto de \textbf{páginas}, \(k \in \Naturals\) é o \textbf{tamanho do cache} e \(\pi \colon \Hcal \to \Zcal\), onde \(\Scal = \binom{\Zcal}{k}\), \(\Hcal = \Seq(\Scal) \times \Seq(\Zcal) \), é uma função, chamada \textbf{política}, tal que, \(\forall (h,z) \in \Hcal\) com \(|\boldh| = m\), então \(\pi((\boldh,\boldz)) \in h_m\).
\end{definition}

Seja \(\Ccal \coloneq (\Zcal, k, \pi)\) uma instância online de caching. Associamos a \(\Ccal\) uma função \(\Acal_\Ccal\) chamada \textbf{algoritmo online de caching} que recebe os seguintes parâmetros:
  \begin{itemize}
    \item \(\boldz \in \Seq(\Zcal)\), que chamamos de \textbf{sequência de pedidos};
    \item \(w \in \Scal\), que chamamos de \textbf{cache inicial}.
  \end{itemize}

É importante alertar que as sequências de pedidos e o cache inicial não serão definidos para pares de sequências de tamanho arbitrário. Em vez disso, eles serão definidos apenas para pares de sequências cujos tamanhos possam surgir no contexto de aprendizado \emph{online} que definiremos mais adiante. No entanto, por conveniência, definimos um oráculo do jogador (ou do adversário) como uma função de \(\Seq(X) \times \Seq(Y)\) (ou \(\Seq(X) \times \Seq(D)\)) para \(D\) (ou \(Y\)).

Defina \(A_\pi\) de forma iterativa como no \Cref{algo:cache}. Para \(i \in \Naturals - \{0\}\), chamamos de \(i\)-ésimo \emph{passo} a iteração do \Cref{algo:cache} na qual é definido o \((i+1)\)-ésimo elemento \(s_{i+1}\) da sequência retornada pelo algoritmo.

\begin{algorithm}
  \caption{Definição de \(\Acal_\pi(w, \boldz)\)}
  \label{algo:cache}
  \begin{algorithmic}[1]
    \Require \(w\) e \(\boldz\), que são um cache inicial e uma sequência de pedidos de tamanho \(n \in \Naturals\), respectivamente.

    %
    \Ensure \(\bolds \in \Scal^n\), onde \(\Scal = \binom{\Zcal}{k}\)
    % 
    \State \(s_1 \gets w\)
    \For{\(i = 1\)~to~\(n\)}
    %
    \If {\(z_i \notin s_{i}\)}
    \State \(s_{i+1} \gets s_{i} - \pi(\angb{s_1, \dotsc, s_{i}}, \angb{z_1, \dotsc, z_{i-1}}) + z_i \) \label{linha}
    \Else
    \State \(s_{i+1} \gets s_i\)
    \EndIf
    \EndFor
    \State \Return \(\bolds\)
  \end{algorithmic}
\end{algorithm}

\newpage

Ao longo do texto o único parâmetro da instância de caching \(C = (\Zcal, k, \pi)\) que será variado é a política \(\pi\); o conjunto abstrato \(\Zcal\) e o tamanho do cache \(k \in \Naturals\) são fixados\footnote{Claro, \(k\) continua sendo uma variável, mas queremos dizer que não trataremos de dois valores distintos para \(k\) em nenhum contexto ao longo do texto}. Assim, a instância é determinada unicamente pela política \(\pi\). Além disso, para manter a notação simples, omitiremos o cache inicial \(w\) dos parâmtros de \(\Acal_\pi\) e fixemos o cache inicial \(s_1\) com algum conjunto arbitrário em \(\binom{\Zcal}{k}\). Essa escolha não é importante pois, conforme veremos, estamos interessados somente no comportamento \emph{assintótico} do problema em que as condições inicias tornam-se irrelevantes. Finalmente, também suprimos a indicação da política \(\pi\) no algoritmo de caching sempre que isso não for importante. Passaremos a falar então somente de um algoritmo de caching \(\Acal\) que determina unicamente a instância do problema e a política \(\pi\) \emph{associada} ao algoritmo.

Chamamos de \textbf{conjunto de políticas de caching} as funções \(\pi\) que respeitam a condição dada na \Cref{def:cache} e o denotamos por \(\Pi\). Dada uma sequência de pedidos \(\boldz\), seja \(\bolds = \Acal_\pi(\boldz)\) para alguma política \(\pi \in \Pi\). Cada \(s_i \in \binom{\Zcal}{k}\)\footnote{Pela definição do cache inicial, o cache começa cheio e, portanto, não nos preocupamos com o caso em que há acertos de cache inevitáveis no começo do processamento devido o cache não estar totalmente cheio}representa o \emph{estado do cache}, ou simplesmente \emph{cache}, no passo \(i\), ou seja, o conjunto das \(k\) páginas na memória rápida imediatamente antes do algoritmo processar o \emph{pedido} \(z_i\) para todo \(i \in [n]\), onde \(n = |\boldz|\). Uma \textbf{falha de cache} ocorre no passo \(i\) se \(z_i \notin s_i\). A política \(\pi\), então, determina uma página \(p_i \in s_i\) para ser \emph{descartada}, tal que \(p_i = \pi(\bolds_{1:i}, \boldz_{1:i})\). Observe que a condição exigida da política na \Cref{def:cache} garante que \(p_i\) está no cache \(s_i\) do passo atual, o que respeita a semântica do problema. A política \(\pi\) recebe como parâmetros o \emph{histórico} dos estados do cache e dos pedidos até o passo atual, que são as sequências \(\bolds_{1:i} = \angb{s_1, \dotsc, s_i}\) e \(\boldz_{1:i} = \angb{z_1, \dotsc, z_i}\), respectivamente. Isto representa o caráter \emph{online} do problema: o algoritmo deve tomar decisões sem conhecimento dos pedidos futuros. Se \(z_i \in s_i\), dizemos que houve um \textbf{acerto de cache} no passo \(i\) ou, de forma equivalente, que o algoritmo \emph{acertou} no passo \(i\) e o cache não é alterado pelo algoritmo.


O objetivo geral do problema de caching é encontrar políticas que resultem em um custo baixo para qualquer entrada. A redução das falhas de cache depende, em grande parte, de manter no cache as páginas que serão requisitadas em breve. Com isso, são formuladas políticas que procuram atingir esse objetivo. Seja \(\Acal\) um algoritmo online para o problema de caching. Dizemos que \(\Acal\) é \textbf{determinístico} se sua política \(\pi\) é determinística, ou seja, não possui um gerador de números aleatórios para auxiliar suas decisões. Por outro lado, um algoritmo \textbf{aleatorizado} pode tomar decisões aleatórias. Conforme veremos adiante, os algoritmos determinísticos possum menos "poder" e os algoritmos aleatorizados conseguem atingir competitividades melhores na teoria (ver seção x para a definição de competitivade de algoritmos aleatorizados). 



\begin{definition}[Custo]
  Seja \(\Pi\) o conjunto de algoritmos online de caching e \(\Scal \coloneq \binom{\Zcal}{k}\). Definimos a função \textbf{custo}, denotada por \(c \colon \Pi \times \Seq(\Zcal) \to \Integers\), dada por:
  \begin{equation}
    c(\Acal_\pi, \boldz) = \sum_{i=1}^{|z|} \ind{z_i \notin s_i}.
  \end{equation}
  onde \(\bolds = \Acal_\pi(\boldz)\). O custo é a quantidade total de falhas de cache. Além disso, \(\forall \boldz \in \Seq(\Zcal)\) denotamos por \(\opt(\boldz)\) o custo mínimo sobre todas as políticas de caching,
  \begin{equation}
    \opt(\boldz) = \inf_{\pi \in \Pi} c(\Acal_\pi, \boldz).
  \end{equation}
\end{definition}

Sempre que o contexto permitir, usaremos a notação \(c(\Acal, \boldz_{j:k}) \coloneqq \sum_{i=j}^{k} \ind{z_i \notin s_i}\) com \(\bolds = \Acal_\pi(\boldz)\) para denotar o custo acumulado entre os passos \(j\) e \(k\). Note que isso não significa que o algoritmo processa apenas a sequência \(\boldz_{j:k}\); a referência é meramente formal para indicar o custo entre esses índices, mantendo o cache inicial fixo. Essa notação relaxada simplifica a exposição sem gerar ambiguidades.

\paragraph*{Funções auxilares sobre a entrada.}Primeiramente, para \(\boldz \in \Seq(\Zcal)\), uma entrada qualquer de tamanho \(n\), definimos a função \textbf{próxima ocorrência} \(\tau \colon \Zcal \to \Naturals\) que associa para índice \(i \in [n]\) e página \(x \in \Zcal\) o índice da sua (possível) próxima ocorrência em \(\boldz\)\footnote{Rigorosamente, essa função deveria receber também a sequência \(\boldz\). No entanto, como só a usamos em um contexto suficientemente claro em que há uma única entrada sendo discutida, omitimos essa dependência por simplicidade.}. Formalmente,

\begin{equation*}
\tau(i, x) =
\begin{cases}
  \min \{ i' > i \mid z_{i'} = x \}, & \text{se tal } i' \text{ existe}, \\
  +\infty, & \text{caso contrário}.
\end{cases}
\end{equation*}

\begin{definition}
  \label{def:blocos}
Definimos a função \(\theta : [n] \times \Zcal \to \mathbb{N}_0\) que associa a cada posição \(j \in [n]\) e elemento \(x \in \Zcal\) o instante da última ocorrência de \(x\) antes de \(j\), por
\begin{equation}
\theta(j, x) =
\begin{cases}
  \max\{ t < j \mid \sigma_t = x \}, & \text{se tal } t \text{ existe}, \\[6pt]
  0, & \text{caso contrário}.
\end{cases}
\end{equation}
Aqui adotamos a convenção de que \(\rho(j,x)=0\) significa que \(x\) não ocorreu antes de \(j\).
\end{definition}

\begin{definition}[Partição em blocos de \(k\) páginas distintas]
  \label{def:fases}
  Antes da definição principal, definimos duas funções auxiliares. Defina a função \(\boldM \colon \Seq(\Zcal) \to \Seq(\Pcal(\Zcal))\)  da seguinte forma: seja \(\boldz \in \Zcal^n\), \(n \in \Naturals\). Então, \(\boldM(\boldz)\) é a sequência de tamanho \(n\) tal que \(M_1 = s_1\) (conjunto arbitrário de \(k\) páginas arbitrárias) e, para todo \(i \in [2,n]\), 
  \begin{equation*}
    M_i =
  \begin{cases}
    M_{i-1} + z_i, & \text{ se } |M_{i-1} + z_i| \le k  \\
  \{z_i\}, & \text{caso contrário}.
\end{cases}
\end{equation*}

Além disso, defina \(\boldl \colon \Seq(\Zcal) \to \Seq(\Naturals)\) tal que, se \(\boldz \in \Seq(\Zcal)^n\), então \(\boldl(\boldz)\) é a sequência de tamanho \(m \le n\) tal que tal que \(l_1 =1\) e \(l_i = \min\{i' > i \colon |M_i| = k \text{ e } |M_{i-1}| = k\}\) para \(i \in [2,m]\) (COMO DEFINIR O FINAL DA SEQUÊNCIA?). Ou seja, \(l_i\) denota o índice em que o conjunto \(M\) é esvaziado. Finalmente, chamamos de \textbf{partição em blocos de \(k\) páginas} de \(\boldz\) a partição contígua (ver seção ) de \(\boldz\) determinada por \(\boldl(z)\), e denotamos por \(\boldb(\boldz)\).
\end{definition}

Após cada requisição \(z_i\), o conjunto \(M_i\) contém todas as páginas distintas observadas desde o último esvaziamento, que ocorre quando uma nova página faz o tamanho de \(M_i\) exceder \(k\). Os índices \(\boldl(\boldz)\) marcam o início de cada bloco, de modo que cada subsequência \(\boldz_{l_t : l_{t+1}-1}\) corresponde a uma fase contendo no máximo \(k\) páginas distintas.

A motivação dessa definição é que será mais fácil contar falhas de cache por blocos.

\begin{definition}
  Seja \(\mathbb{A}\) o conjunto de algoritmos determinísticos. Um \textbf{algoritmo aleatorizado} \(A_r\) para o problema de caching é uma variável aleatória sobre \(\mathbb{A}\). 
\end{definition}

Analogamente ao que foi feito para algortimos determinísticos, podemos definir o coeficiente de competitividade para algortimos aleatorizados. A única diferença na definição surge no fato que, agora, o custo \(Custo(R, \sigma)\) é uma variável aleatória.

\subsection{Competitividade para algortimos aleatorizados}

\begin{definition}
  Dizemos que um algoritmo aleatorizado \(\Acal\) para o problema de caching é \(c\)-competitivo se para toda sequência \(z \in \Seq(\Zcal)\) existe uma constante \(\alpha\) tal que
  \begin{equation}
    \mathbb{E}[c(\Acal, z)] \le c \opt(z) + \alpha
  \end{equation}
  
\end{definition}




\subsection{Algoritmo offline ótimo}

A \Cref{def:cache} define a política \(\pi\) de forma geral, apenas impondo que deva escolher sempre uma página presente no cache atual. O comportamento \emph{online} é, de fato, estabelecido pela definição do algoritmo \(\Acal_\pi\) em \Cref{algo:cache}. Em um algoritmo online \(\Acal_\pi\), a política \(\pi\) utiliza somente o histórico dos pedidos \(\boldz_{1:i}\), sendo incapaz de enxergar pedidos futuros. Em contraste, um algoritmo \textbf{offline} segue a mesma estrutura de \Cref{algo:cache}, com a única diferença crucial de que a política \(\pi\) tem acesso à \emph{sequência de pedidos completa} \(\boldz\) em qualquer passo \(i\). Isso significa que, na linha de descarte (Linha~\ref{linha}), a política recebe parâmetros \(\bolds_{1:i}\) e \(\boldz\), conferindo-lhe um poder preditivo. Para problemas intrinsecamente *online*, como o caching, algoritmos offline não são implementáveis na prática. Contudo, são essenciais como base comparativa para os algoritmos online, conforme veremos.

Um algoritmo offline \(\Acal\) é \textbf{ótimo} se \(\forall \boldz \in \Seq(\Zcal)\), \(c(\Acal, \boldz) = \opt(\boldz)\). \textcite{Belady66} propôs um simples algoritmo offline ótimo \(\Acal_{\bold{FIF}}\) que implementa uma política denominada \textit{Furthest-in-the-Future}, denotada por \(\pi_{\mathbf{FIF}}\). Essa política descarta a página do cache que permanecerá sem pedidos pelo maior tempo no futuro. Definemos a política de forma explícica. 
Assim, a política \( \pi_{\mathbf{FIF}} \) é definida por, \( \pi_{\mathbf{FIF}} (\bolds_{1:i}, \boldz) = \arg {\max_{x \in s_i} \tau(i, x)}\). A prova da otimalidade de \(\Acal_{\pi_{\mathbf{FIF}}}\) não é trivial, e apresentamos uma demonstração no apêndice.

\subsection{Análise competitiva}

\textcite{Sleator93} propôs uma forma alternativa de análise de algoritmos online, chamada de \emph{análise competitiva}, em que o o desempenho de um algoritmo online é determinado pela razão entre seu custo e o custo do algoritmo offline\footnote{A partir daqui dispensaremos a palavra "ótimo": sempre nos referimos ao algoritmo offline ótimo} no pior caso.
\begin{definition}
  \label{def:comp}
  Seja \(\Acal\) um algoritmo online de caching. Dizemos que \(\Acal\) é \(c\)\textbf{-competitivo} se existe uma constante \(\alpha\) tal que para todo \(\boldz \in \Seq(\Zcal)\),
\begin{equation}
  c(\Acal, \boldz ) \le c \opt(\boldz) + \alpha.
\end{equation}
\end{definition}

Quando a \textbf{constante aditiva} \(\alpha\) é menor ou igual a zero (i.e., \(c(\Acal, \boldz) \le c \opt(\boldz)\)) podemos dar ênfase e dizer que \(\Acal\) é \textbf{estritamente \(c\)-competitivo}. Permitir \(\alpha > 0\) reflete o fato que para problemas intrinsecamente online como o problema de caching, existe uma entrada \(\boldz\) arbitrariamente longa que faz \(c(\Acal, \boldz)\) ser ilimitado. A constante \(\alpha\) torna-se insignificante a medida que consideramos entradas mais longas. Além disso, mesmo para entradas finitas, o uso de \(\alpha\) permite definir uma razão de desempenho que não depende das condições iniciais, neste caso, o cacho inicial. 

Para cada entrada \(\boldz\), um algoritmo \(c\)-competitivo é garantido de ter um custo dentro do fator \(c\) comparado com o custo mínimo \(\opt(\boldz)\) (a menos da constante aditiva \(\alpha\)). A razão de competitivade é sempre maior ou igual a \(1\) e quanto mais próxima de 1, melhor o algoritmo desempenha comparado com o algortimo offline ótimo que atinge sempre custo \(\opt(\boldz)\).

Se \(\Acal\) é \(c\)-competitivo, dizemos que o algoritmo \emph{atinge} uma \textbf{razão de competitivade} \(c\). Um algoritmo é chamado \textbf{competitivo} se ele atinge uma razão competitiva \(c\) "constante", ou seja, \(c\) não depende da entrada \(\boldz\) mas pode depender do parâmetro do problema (em geral, o tamanho do cache \(k\)). O ínfimo sobre o conjunto de todos os valores de \(c\) tal que \(\Acal\) é competitivo é chamado de \textbf{a razão competitiva} de \(\Acal\) denotada por \(\Rcal(\Acal)\).

\section{Algortimos determinísticos}

A política \(\pi_{\mathbf{LRU}}\) (\textit{Least Recently Used}), descarta a página no cache cuja última ocorrência é a menor, um indicativo que não aparecerá tão breve, pois há muito tempo não foi requisitada. Formalmente, \( \pi_{\mathbf{LRU}} (\bolds_{1:i}, \boldz) = \arg {\max_{x \in s_i} \theta(i, x)}\)

No algortimo \(\mathbf{A_{FIFO}}\) (\textit{First-in, First-out}), \(e(i)\) é o elemento que está há mais tempo no cache. 

Aqui, não impomos nenhum requisito ou suposição sobre a eficiência computacional dos algoritmos de caching: nosso foco é o custo do algoritmo, isto é, a quantidade total de falhas de cache, que independente da exata implementação de cada estratégia nos algoritmos.

Foi dito que existe um algoritmo determinístico ótimo offline (que implementa a política \textbf{FIF}) que incorre o custo mínimo para uma entrada \(z\). A dúvida natural que surje é quão próximo do custo mínimo um algoritmo online pode alcançar. Veremos nas próximas seções que nenhum algoritmo online pode igualar esse custo para toda entrada, como é esperado. De fato, somente um algoritmo online que pudesse advinhar os pedidos futuros poderia escolher, em cada passo, o exato elemento \(e(i)\) que minimiza globalmente o custo para uma entrada. Por isso, imaginamos \(\opt(z)\) como um custo intrínseco da entrada. Assim, há uma limitação para os algoritmos online, tanto determinísticos quanto aleatorizados, e veremos em que medida são nas seções seguintes atráves de suas competitividades. (MELHORAR REDAÇÂO)

\subsection{Cota inferior para a competitividade}

Primeiramente, provamos uma cota inferior para a razão competitiva dos algoritmos determinísticos, mostrando o quão próximos podem ser do algoritmo offline.

\begin{theorem}
\label{teo:cota_det}
Seja \(\Acal\) um algoritmo online de caching determinístico. Então, \(\Rcal(\Acal) \geqslant k\).
\end{theorem}

\begin{proof}

  Pela \Cref{def:comp} devemos provar que \(\Acal\) é no mínimo \(k\)-competitivo. Para isso, basta provar que existe uma entrada \(\boldz\) arbitrariamente grande tal que \(\frac{c(\Acal,\boldz)} {\opt(\boldz)}\) é no mínimo \(k\). 

  Podemos construir indutivamente uma sequência patológica \(\boldz\) com qualquer tamanho \(n\) tal que \(c(\Acal, \boldz) = n\). De fato, basta que o pedido em um passo seja precisamente a página descartada deterministicamente pelo algoritmo no passo anterior; tomamos \(z_1\) igual a qualquer página fora do cache inicial e \(z_i = \pi(\bolds_{1:i}, \boldz_{1:i-1})\) para \(i = 2, \dotsc, n\), onde \(\pi\) é a política associada a \(\Acal\).

  Usemos a política \(\pi_{\mathbf{FIF}}\) para calcular \(\opt(\boldz)\). Pela descrição acima, fica claro que \(z_i \in s_1 \cup \{z_1\}\) e, portanto, a entrada envolve exatamente \(k+1\) páginas distintas. Pelo princípio da casa dos pombos, nenhum algortimo processa \(\boldz\) sem falhar\footnote{Na verdade, no primeiro passo já acontece uma falha pois \(z_1 \notin s_1\): na nossa formalização todo algoritmo começa com o mesmo cache inicial. Mas esse fato, obviamente, é mais geral e é verdade independentemente desta primeira falha.}. Suponha, então, que o algoritmo offline falha no passo \(j\), descartando a página \(d\). Como apenas uma página está fora do cache em cada passo, a próxima falha ocorrerá necessariamente na página \(d\), no passo \(\tau(j,d)\), resultando em \(\tau(j,d) - j - 1\) acertos consecutivos entre as falhas. Além disso, de \(d = \pi_{\mathbf{FIF}}(\bolds_{1:j}, \boldz_{1:j-1}) = \arg\max_{p \in s_j} \tau(j, p)\) segue que \(\tau(j, d) > j + k - 1\), pois as \(k - 1\) páginas em \(s_j - \{p\}\) têm próxima ocorrência (com valores distintos) menor. Em resumo, após uma falha de cache, o algoritmo offline acerta ao menos \(k-1\) vezes antes da próxima falha. Logo, o número de acertos de cache \(n-\opt(\boldz)\) é no mínimo \(\opt(\boldz)(k-1)\). Daí, \(\opt(\boldz) \ge nk = c(\Acal,\boldz)k\) e \(\frac{c(\Acal,\boldz)}{\opt(\boldz)} \le k\).

\end{proof}

\subsection{Competitividade de \(\lru\)}

Dada a cota inferior \(k\) para a razão competitiva de qualquer algoritmo determinístico, avaliamos quão próximos desse limite os algoritmos clássicos podem atingir. Primeiramente, provamos que o algoritmo \(\lru\) atinge esse limite, ou seja, \(\Rcal(\lru) = k\).

\begin{lemma}
  \label{lem:k-part}
  Sejam \(\Acal\) um algortimo online de caching, \(\boldz \in \Zcal^n\) e considere \(\boldb(z)\), uma partição em blocos de \(k\) falhas para \(\Acal\); \(\boldl = \angb{l_1, \dotsc, l_m}\) determina os índices do começo de cada bloco. Assim, \(z_{l_j}\) é a primeiro elemento de \(b_j\) e \(z_{l_{j+1}-1}\) é o último\footnote{Para \(j=m\), o último elemento é \(z_n\)}. Seja \(q = z_{l_j-1}\) o último elemento do bloco anterior \(b_{j-1}\). Se \(\Acal = \lru\), então \(b_j\) contém \(k\) páginas distintas que são diferentes de \(q\). 

  \begin{proof} Olhemos para páginas referenciadas nas \(k\) falhas durante \(b_j\), então, há três casos: I) todas as páginas são distintas e diferentes de \(q\); II) nem todas as páginas são distintas; III) todas as páginas são distintas mas nem todas são diferentes de \(q\). Provemos que o lema vale para cada caso.

  I) Nada a provar, o lema segue diretamente.

  II) Suponha, então, que \(\lru\) falha duas vezes para um elemento \(r \in b_j\) (?) nos passos \(i_1\) e \(i_2\) com \(l_j \le i_1 < i_2 \le l_{j+1} - 1\). Para isso, \(r\) foi descartado em algum passo intermediário \(i_1 < i < i_2\). De \(r = \pi_{\mathbf{LRU}}(\bolds_{1:i}, \boldz_{1:i-1}) = \arg\min_{x \in s_i} \theta(i, x)\) e \(\theta(i, r) = i_1\) segue que cada página em \(s_j - \{r\}\) foi referenciada em algum tempo em \([i_1+1, i-1] \subset [l_j, l_{j+1}-1]\). Assim, o conjunto \(s_j \cup \{z_i\}\) é formado por \(k+1\) páginas distintas que foram referenciadas durante \(b_j\). Daí, segue necessariamente que ao menos \(k\) destas são diferentes de \(q\).

  III) Suponha, então, que \(\lru\) falha em algum tempo \(l_j \le i \le l_{j+1}-1\) com \(z_i = q\). Antes de processar o primeiro pedido da fase \(b_j\), \(q\) está no cache pois \(z_{l_j-1} = q\). Usando um argumento análogo ao usado no caso II), concluímos que \(\{z_{l_j-1}, z_{l_j}, \dotsc, z_i\}\) contém \(k\) páginas distintas. Então, como \(z_{l_j-1} = q\), há exatamente \(k\) páginas distintas e diferentes de \(q\) em \(\{z_{l_j}, \dotsc, z_i\} \subset b_j\).

\end{proof}
\end{lemma}

Finalmente, provamos que a \(\lru\) atinge a melhor competitivade para algoritmos determinísticos dada pelo \Cref{teo:cota_det}.

\begin{theorem}
  A competitividade do algoritmo \(\lru\) é igual ao tamanho do cache. Isto é, \(\Rcal({\lru}) = k\).
\end{theorem}

\begin{proof}
  Seja \(\boldz \in \Zcal^n\) uma entrada qualquer e considere uma partição de blocos de \(k\) falhas de \(\lru\). Pela \Cref{def:blocos}, dois blocos consecutivos envolvem ao menos \(k+1\) páginas distintas, caso contrário formariam apenas um bloco. Pelo princípio da casa dos pombos, o algoritmo offline, portanto, não poderá processar esses elementos sem falhar ao menos uma vez. Como isso vale para qualquer par de blocos consecutivos \(\opt(z) \ge m-1\). 

  Após uma falha de cache, \(\lru\) remove a 

    \begin{align*}
    \lru(z) 
    &= \sum_{j=1}^m c(\lru,b_j) 
    \\
    &= c(\lru,b_1) + \sum_{j=2}^m c(\lru,b_j)
    \\
    &= c(\lru,b_1) + (m-1)k \le k + (m-1)k = mk.
    \end{align*}

    onde a última linha segue pela definição de \(\boldb\). Logo, para toda entrada \(z\) de tamanho \(n\), \(\frac{\lru(z)}{\opt(z)} \le k\frac{m}{m-1}\). Facilmente podemos imaginar uma entrada com número de blocos arbitrariamente grande, onde \(\frac{m}{m-1} \to 1\) e, então, \(\frac{\lru(z)}{\opt(z)} \le k\). Pela existência dessa entrada e pelo \Cref{teo:cota_det}, segue o teorema.
\end{proof}

COLOCAR COMENTÁRIOS SOBRE ISSO (ver artigo)

\subsection{Competitividade de \(\Acal_{\mathbf{FIFO}}\)}

\subsection{Prova de otimalidade da \(\Acal_{\mathbf{FIF}}\)}

Finalmente, para de fato provar os algoritmos dessa seção, vamos provar a otimalidade do algoritmo em \textcite{Belady66}, que implementa a política de descartar o elemento com pedido mais afastado no futuro.


\section{Algoritmos aleatorizados}

\subsection{Adversários e jogos}

\subsection{Cota inferior para a competitivade}

O teorema \ref{teorema1} diz que nenhum algoritmo determinístico tem coeficiente de competitividade menor que \(k\). Quão melhor podem se sair os algoritmos aleatorizados de caching?

Sejam \(\Dcal\) o conjunto de algoritmos online determinísticos de caching e \(\Xcal = Seq(\Zcal)\) o conjunto de entradas e \(\Acal\) um algoritmo aleatorizado. O \Cref{teo:minimax} afirma que 
\begin{equation}
  \label{eq:minimax}
    \max_{z \in \Xcal} \Expect[c(\Acal,z)] = \min_{A \in \Dcal} \Expect[c(A,X)]
\end{equation}

\begin{theorem}[Cota inferior para algoritmos aleatorizados]
  Seja \(\Acal\) um algoritmo aleatorizado de caching. Então, \(\Rcal(\Acal) \ge H_k\) onde \(H_k = \sum_{j = 1}^{k} 1/j\) é o \(k\)-ésimo número harmônico.
\begin{proof}
  Pela \Cref{eq:minimax} e pela discussão acima, para mostrar que nenhum algoritmo aleatorizado atinge competitividade \(c\), basta mostrar uma variável aleatória \(\boldZ\) tomando valores em \(\Zcal\) tal que, para todo \(\Acal \in \Dcal\),
  \begin{equation*}
    \Expect\left[\frac{c(\Acal, \boldZ)}{\opt(\boldZ)}\right] > c.
  \end{equation*}

  Primeiramente, tome \(Z'\) qualquer em \(\binom{\Zcal}{k}\). Sejam \(X_1, X_2, \dotsc\) variáveis aleatórias independentes tal que \(X_i\) é tomado uniformemente em \(\Zcal'\). Defina, \(L = \inf\{T \colon z \in \boldX_{1:T} \text{ para todo } z \in \Zcal'\}\) e \(\boldZ = X_{1:L}\), ou seja, a entrada é construída tomando páginas uniformemente em \(\Zcal'\) até que todas estejam incluídas na entrada, o resultado é uma entrada com tamanho aleatório \(L\). Como \(\boldZ\) contém apenas um bloco, \(\opt(\boldZ) = 1\). Agora, seja \(\Acal\) um algoritmo determinístico, mostraremos que \(\Expect[c(\Acal,\boldZ)] = H_{k+1}\). Uma falha de cache acontece no passo \(i\) se \(z_i \in \Zcal' \setminus s_i\), onde \(\bolds = \Acal(\boldZ)\). Esse último conjunto tem apenas um elemento em todo passo e esse evento é independente e identificamente distribuídos, portanto, \(\Pr\{\ind{Z_i \notin s_i} = 1\} = 1/(k+1)\). Pela identidade de Wald (\Cref{lema:wald}), temos que,
  \begin{align*}
    \Expect[c(\Acal, \boldZ) = \Expect\left[\sum_{i=1}^L \ind{Z_i \notin s_i}\right]
    &= \Expect[L]\frac{1}{k+1} = H_{k+1}
  \end{align*}

onde a ultima igualdade segue pelo \Cref{teo:cupon}.

\end{proof}
\end{theorem}

\subsection{Algortimos de marcação}

Os algoritmos de marcação são uma família de algoritmos de caching que seguem a seguinte estrutura. A execução do algoritmo é em fases, e no começo de cada fase, cada elemento do cache é dito \emph{desmarcado}. Quando um acerto de cache ocorre, o elemento correspondente é \emph{marcado}. Quando uma falha de cache acontece, alguma página desmarcada é descartada do cache, e o novo elemento é colocado no cache e imediatamente marcado. Se todas as páginas do cache estão marcadas e outra falha de cache acontece, então todo o cache é desmarcado e uma nova fase começa. Assim, um \textbf{algoritmo de marcação} é um algoritmo de caching que nunca descarta páginas marcadas, e seu funcionamente é resumido pela forma que escolhe páginas desmarcadas para descartar após uma falha de cache. 

Pela definição informal dada acima, fica claro que o algoritmo opera em blocos de \(k\) páginas distintas e o conjunto de páginas marcadas a cada passo é \(M_i\) (veja \Cref{def:blocos}).  Um algoritmo online de caching \(\Acal\) é chamado de \textbf{algoritmo de marcação} se sua política \(\pi\) só escolhe elementos desmarcados:
\begin{equation*}
\pi(\angb{s_1, \dotsc, s_{i}}, \angb{z_1, \dotsc, z_{i-1}}) \in (s_i \setminus M_i).
\end{equation*}

Abaixo, definimos noções úteis para a análise de algoritmos de marcação. Fixe \(\boldz \in \Seq(\Zcal)\) e \(\boldb = \boldb(\boldz)\) seu conjunto de blocos nas definições abaixo.

\begin{definition}
  Seja  \(p \in b_r\), chamamos \(p\) de um pedido \textbf{novo} se \(p \notin b_{r-1}\) ou \textbf{velho} se \(p \in b_{r-1}\). A \textbf{chegada} de \(p\) é o passo em que é referenciada pela primeira vez na fase \(r\).
\end{definition}

Observe que \(p \notin b_{r}\) é equivalente a \(p \notin M_{l_r-1}\), pois \(M_{l_r-1}\) é o conjunto de páginas marcadas no bloco \(r-1\).

Então, em um bloco, cada pedido \(z_i\) ou é velho ou é novo. Se \(z_i\) foi referenciado no bloco anterior, é velho; novo, caso contrário. No último passo \(j\) de um bloco \(r\), as páginas referenciadas durante esse bloco estão todas marcadas e no cache pois após suas chegadas são marcadas e não saem mais, logo, \(M_j = s_j\). A chegada de uma página distinta daqules em \(s_j\) faz zerr \(M\) e começar um novo bloco. Isso para o algoritmo de marcação. Observe que os conjuntos \(M_i\) e consequentemente os blocos \(\boldb\) são independentes do algoritmo em questão e são totalmente determinados dado \(\boldz\).

\begin{fact}
  \label{fato:nova}
  Para um algoritmo de marcação, toda chegada de um pedido novo \(z_i\) resulta em uma falha de cache, pois \(z_i \notin M_{r-1} = s_{r-1}\).
\end{fact}

\begin{lemma}
  \label{lem:opt_nova}
  Seja \(L\) o número total de pedidos novos em \(\boldz\). Então, \(\opt(\boldz) = \Theta(L)\). 
 \begin{proof} Suponha \(|\boldb| = m > 2\) e denote por \(l_r\) o início do bloco \(b_r\), de modo que, \(b_r = \angb{z_{l_r}, \dotsc, z_{l_{r+1}-1}}\).  
   Seja \(L_r\) o número de pedidos novos na fase \(r\). Defina a sequência \(\boldd\), de tamanho \(m+1\), por \(d_r = |s_{l_r} - M_{l_r-1}|\) para \(r \in [m]\), isto é, a diferença entre o cache de \(\opt\) no início do bloco \(r\) e o conjunto \(M_{l_r-1}\) das \(k\) páginas referenciadas no bloco anterior. A seguir, usamos a sequência \(\boldd\) para chegar em duas cotas inferiores para o custo \(\opt(b_r)\). 

   i) Pela definição de \(d_r\), no passo \(l_r\), \(\opt\) têm no máximo \(d_r\) páginas novas no cache. Em um algoritmo de marcação, esse número seria zero mas \(\opt\) tenta prever alguns dos pedidos novos do bloco atual e por isso já tem alguns deles no cache. Portanto, deixará de fora ao menos \(L_r - d_r\) páginas novas, cada uma delas resulta em uma falha: \(\opt(b_r) \ge L_r - d_r\). ii) Pelo mesmo motivo, no final do bloco, têm no cache \(d_{r+1}\) páginas que não são referenciadas no bloco \(r\). Estas páginas formam, junto com o conjunto \(M_{l_{r+1}-1}\), \(k+d_{r+1}\) páginas distintas que estão no cache (de tamanho \(k\)) em pelo menos um passo do bloco \(r\). Pelo princípio da casa dos pombos, então, há de haver ao menos \(k+d_{r+1}-k = d_{r+1}\) trocas durante o bloco, isto é, \(\opt(b_r) \ge d_{r+1}\). Assim,  
\begin{equation*}
  \opt(b_r) \ge \max\{L_r - d_r, d_{r+1}\} \ge \frac{L_r - d_r + d_{r+1}}{2},
\end{equation*}
onde a segunda desigualdade usa o fato de que, para quaisquer \(a,b \in \Reals\), \(\max(a,b) \ge \frac{a+b}{2}\). Somando sobre todos os blocos:
\begin{equation*}
  \opt(\boldz)
    = \sum_{r=1}^m \opt(b_r)
    \ge \frac{1}{2} \sum_{r=1}^m L_r + \frac{1}{2}\sum_{r=1}^m (d_{r+1} - d_r)
    = \frac{L}{2} + d_{m+1} - d_{m-1} + d_1
    = \frac{L}{2} + O(k),
\end{equation*}
pois a segunda soma telescopa e \(d_i \le k\). FALTA A OUTRA DIREÇÂO.
\end{proof}
\end{lemma}

\begin{definition}[Algoritmo \textbf{Marker}]
O algoritmo de marcação mais simples é o que escolhe uniformemente um elemento desmarcado para sair do cache. Denotamos esse algoritmo \textbf{Marker} por \(\Acal_{\mathbf{MK}}\). Formalmente,  
\begin{equation*}
\pi_{\mathbf{MK}}(\angb{s_1, \dotsc, s_{i}}, \angb{z_1, \dotsc, z_{i-1}}) \sim \mathrm{Unif}(s_i \setminus M_i).
\end{equation*}
\end{definition}

\begin{theorem}[Competitividade de \(\Acal_{\mathbf{MK}}\)] O algoritmo \(\Acal_{\mathbf{MK}}\) atinge competitivade \(2H_k\), isto é, \(\Rcal(\Acal_{\mathbf{MK}}) \le 2H_k\).
\begin{proof}
  Seja \(\boldz \in \Seq(\Zcal)\) e \(b_j\) um bloco qualquer de \(\boldz\). Durante o bloco, há dois Além disso, suponha que essa fase teve \(L\) páginas novas. Pelo \Cref{lem:nova}, estas \(L\) páginas incorrem \(L\) faltas.
  Para as novas, ordene as \(k-L\) chegadas por ordem de aparição \(j=1,\dots,k-c_r\). No passo da \(j\)-ésima tal requisição, existem exatamente \(k-j+1\) páginas ainda não substituídas desde o início da rodada, das quais \(L\) são limpas (já trazidas e marcadas) e \(k-L-(j-1)\) são obsoletas ainda presentes. Como \(\Acal_{\mathbf{MK}}\) sempre descarta uniformemente entre as não marcadas, a probabilidade de falha nessa \(j\)-ésima requisição velha é no máximo 
\begin{equation*}
\frac{c_r}{\,k-j+1\,}.
\end{equation*}

Somando sobre \(j=1,\dots,k-L\) obtemos o número esperado de falhas devido a páginas velhas:
\begin{equation*}
\sum_{j=1}^{k-L} \frac{L}{k-j+1}
= L \!\!\sum_{t=L}^{k} \frac{1}{t}
= L\big(H_k - H_{L-1}\big)
\;\le\; L H_k.
\end{equation*}

Logo o custo esperado total na rodada \(r\) é
\begin{equation*}
L \;+\; L(H_k - H_{L-1}) \;\le\; L H_k.
\end{equation*}

Do \Cref{lem:opt_nova}, segue o teorema.
\end{proof}
\end{theorem}

\textcite{Dimitris00} mostrou a razão competitiva exata: \(\Rcal(\Acal_{\mathbf{MK}}) = 2H_{k}-1\).

\section{Comentários finais}

MOTIVAÇÂO PARA ALGORITMOS COM PREDIÇÔES E O QUE FOI FEITO NA LITERATURA
