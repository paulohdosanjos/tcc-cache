% LTeX: language=pt-BR
\chapter{Problema de caching tradicional}

O problema de \emph{caching} considera um sistema com dois níveis de memória: uma memória rápida ou cache de tamanho \(k\) e uma memória lenta de tamanho \(m \gg k\). Uma sequência de itens (páginas), chamada de sequência de pedidos, é fornecida como entrada ao \emph{algoritmo de caching} e deve ser servida em ordem. Se a página solicitada estiver no cache, o pedido é satisfeito com custo zero e dizemos que houve um \emph{acerto de cache}. Se a página solicitada não estiver no cache e sim na memória lenta, acontece uma \emph{falha de cache}. Nesse caso, o algoritmo traz a página da memória lenta e coloca no cache antes de satisfazer o pedido. Se o cache estiver cheio, uma página no cache deve ser descartada. O algoritmo, então, escolhe uma página a ser descartada de acordo com uma regra de substituição ou \emph{política}. O objetivo do problema é encontrar uma política que minimize a quantidade de falhas de cache. Antes de entrarmos nos modelos de predições, estudaremos o problema tradicional, ou seja, sem predições.

%resumo do capítulo também

É dado um universo abstrato \(\Zcal\) (\(|\Zcal| \gg k\)), com elementos denominados \emph{páginas}. Uma sequência de \emph{pedidos} \(\boldz\) de tamanho \(n\) é fixada, onde cada elemento \(z[i]\) é uma página. Os pedidos chegam um por vez nos passos \(i \in [n]\) do algoritmo de caching, que deve satisfazê-los conforme a descrição informal dada acima. O algoritmo é munido de uma função \(\pi\), chamada \emph{política}, que escolhe qual página descartar caso ocorra uma \emph{falha de cache}, isto é, caso \(z[i]\) não esteja no cache. Após escolher e decartar uma página, o algoritmo coloca o pedido \(z[i]\) no cache. O algoritmo de caching munido de uma política \(\pi\) é denotado por \(\Acal_\pi\). O processamento se inicia com um cache inicial \(C \subset \Zcal\) de \(k\) páginas e a saída \(\Acal_\pi(C,\boldz)\) do algoritmo é a quantidade total de falhas de cache. Abaixo está definido o funcionamento de \(\Acal_\pi\), dados \(\pi\), \(C\) e \(\boldz\).

\section{Formalização do problema}
Tendo como base a descrição informal dada acima, definimos mais formalmente o problema de caching. Como é de costume na literatura que trata desse problema sobre a pespectiva do custo de falhas de cache, não será necessária uma notação muito rigorosa e, por isso, definimos o problema a partir de uma notação simples e suficientemente clara para nossos propósitos.

\begin{definition}[Problema de caching]
  \label{def:cache}
  Uma \textbf{instância de caching} é definida pela tripla \((\Zcal, k, \pi) \), onde \(\Zcal\) é um conjunto arbitrário chamado de conjunto de \textbf{páginas}, \(k \in \Naturals\) é o \textbf{tamanho do cache} e \(\pi \colon \Hcal \to \Zcal\), onde \(\Scal = \binom{\Zcal}{k}\), \(\Hcal = \Seq(\Scal) \times \Seq(\Zcal) \), é uma função, chamada \textbf{política}, tal que, \(\forall (h,z) \in \Hcal\) com \(|h| = m\), então \(\pi((h,z)) \in h_m\).
\end{definition}

Seja \(\Ccal \coloneq (\Zcal, k, \pi)\) uma instância online de caching. Associamos a \(\Ccal\) uma função \(\Acal_\Ccal\) chamada \textbf{algoritmo online de caching} que recebe os seguintes parâmetros:
  \begin{itemize}
    \item \(\boldz \in \Seq(\Zcal)\), que chamamos de \textbf{sequência de pedidos};
    \item \(w \in \Scal\), que chamamos de \textbf{cache inicial}.
  \end{itemize}

É importante alertar que as sequências de pedidos e o cache inicial não serão definidos para pares de sequências de tamanho arbitrário. Em vez disso, eles serão definidos apenas para pares de sequências cujos tamanhos possam surgir no contexto de aprendizado \emph{online} que definiremos mais adiante. No entanto, por conveniência, definimos um oráculo do jogador (ou do adversário) como uma função de \(\Seq(X) \times \Seq(Y)\) (ou \(\Seq(X) \times \Seq(D)\)) para \(D\) (ou \(Y\)).

Defina \(A_\pi\) de forma iterativa como no \Cref{algo:cache}. Para \(i \in \Naturals \setminus \{0\}\), chamamos de \(i\)-ésimo \emph{passo} a iteração do \Cref{algo:cache} na qual é definido o \((i+1)\)-ésimo elemento \(s_{i+1}\) da sequência retornada pelo algoritmo.

\begin{algorithm}
  \caption{Definição de \(\Acal_\pi(w, \boldz)\)}
  \label{algo:cache}
  \begin{algorithmic}[1]
    \Require \(w\) e \(\boldz\), que são um cache inicial e uma sequência de pedidos de tamanho \(n \in \Naturals\), respectivamente.

    %
    \Ensure \(\bolds \in \Scal^n\), onde \(\Scal = \binom{\Zcal}{k}\)
    % 
    \State \(s_1 \gets w\)
    \For{\(i = 1\)~to~\(n\)}
    %
    \If {\(z_i \notin s_{i}\)}
    \State \(s_{i+1} \gets s_{i} - \pi(\angb{s_1, \dotsc, s_{i}}, \angb{z_1, \dotsc, z_{i}}) + z_i \) \label{linha}
    \Else
    \State \(s_{i+1} \gets s_i\)
    \EndIf
    \EndFor
    \State \Return \(\bolds\)
  \end{algorithmic}
\end{algorithm}

\newpage

Ao longo do texto o único parâmetro da instância online de caching \(C = (\Zcal, k, \pi)\) que será variado é a política \(\pi\); o conjunto \(\Zcal\) e o tamanho do cache \(k \in \Naturals\) são fixados, e faremos referência aos dois livremente. Assim, a instância é determinada unicamente pela política \(\pi\). Por esta razão, doravante denotaremos o algoritmo de caching associado à instância simplesmente por \(\Acal_{\pi}\), e usaremos esta notação para nos referir, de forma conversível, tanto ao algoritmo quanto à instância definida por essa política.

Chamamos de \textbf{conjunto de políticas de caching} as funções \(\pi\) que respeitam a condição dada na \Cref{def:cache} e o denotamos por \(\Pi\). Dados um cache inicial \(w\) e uma sequência de pedidos \(\boldz\), seja \(\bolds = \Acal_\pi(w, \boldz)\) para alguma política \(\pi \in \Pi\). Cada \(s_i \in \binom{\Zcal}{k}\)\footnote{Supomos que o cache sempre está cheio e não nos preocupamos com os acertos de cache inevitáveis no começo do processamento quando o cache pode estar não totalmente cheio}representa o \emph{estado do cache}, ou simplesmente \emph{cache}, no passo \(i\), ou seja, o conjunto das \(k\) páginas na memória rápida imediatamente antes do algoritmo processar o \emph{pedido} \(z_i\) para todo \(i \in [n]\), onde \(n = |\boldz|\). Uma \emph{falha de cache} ocorre no passo \(i\) se \(z_i \notin s_i\). A política \(\pi\), então, determina uma página \(p \in s_i\) para ser descartada, tal que \(p = \pi(\bolds_{1:i}, \boldz_{1:i})\). Observe que a condição exigida da política na \Cref{def:cache} garante que \(p\) está no cache \(s_i\) do passo atual, o que respeita a semântica do problema. A política \(\pi\) recebe como parâmetros o \emph{histórico} dos estados do cache e dos pedidos até o passo atual, que são as sequências \(\bolds_{1:i} = \angb{s_1, \dotsc, s_i}\) e \(\boldz_{1:i} = \angb{z_1, \dotsc, z_i}\), respectivamente. Isto representa o caráter \emph{online} do problema: o algoritmo deve tomar decisões sem conhecimento dos pedidos futuros. Se \(z_i \in s_i\), houve um \emph{acerto de cache} no passo \(i\) e o cache não é alterado pelo algoritmo.

\begin{definition}
  Sejam \(\Pi\) o conjunto de políticas de caching e \(\Scal \coloneq \binom{\Zcal}{k}\). Definimos a função \textbf{custo}, denotada por \(c \colon \Pi \times \Scal \times \Zcal \to \Integers\), dada por:
  \begin{equation}
    c(\pi, w, z) = \sum_{i=1}^{|z|} \ind{z_i \notin c_i}.
  \end{equation}
  onde \(\boldc = \Acal\pi(w,z)\). O custo é a quantidade total de falhas de cache. Além disso, \(\forall z \in \Seq(\Zcal)\) denotamos por \(\opt(z)\) o custo mínimo
  \begin{equation}
    \opt(z) = \inf_{\pi \in \Pi, w \in \Scal} c(\pi, w, z).
  \end{equation}
\end{definition}

   A partir de agora, nosso foco reside no comportamento \emph{assintótico} dos algoritmos de caching. Para sequências de pedidos \(\boldz\) suficientemente longas, o custo é dominado pelo número de falhas e o estado inicial do cache \(w\) torna-se irrelevante.
Portanto, relaxaremos a notação para omitir o cache inicial \(w\) das entradas. Doravante, usaremos:
    \begin{itemize}
        \item \(c(\pi, \mathbf{z})\) para denotar o custo total de falhas.
        \item \(\opt(\mathbf{z}) = \inf_{\pi \in \mathbf{\Pi}} c(\pi, \mathbf{z})\) para o custo ótimo.
        \item \(\mathbf{A}_\pi(\mathbf{z})\) para a sequência de estados de cache resultante.
    \end{itemize}
Esta convenção será mantida para todas as análises subsequentes, a menos que seja explicitado o contrário.

\subsection{Algoritmo offline ótimo}

A \Cref{def:cache} define a política \(\pi\) de forma geral, apenas impondo que deva escolher sempre uma página presente no cache atual. O comportamento \emph{online} é, de fato, estabelecido pela definição do algoritmo \(\Acal_\pi\) em \Cref{algo:cache}, o que naturalmente leva à separação entre dois tipos de algoritmos. Em um algoritmo online \(\Acal_\pi\), a política \(\pi\) utiliza somente o histórico dos pedidos \(\boldz_{1:i}\), sendo incapaz de enxergar pedidos futuros. Em contraste, um algoritmo \textbf{offline} segue a mesma estrutura de \Cref{algo:cache}, com a única diferença crucial de que a política \(\pi\) tem acesso à \emph{sequência de pedidos completa} \(\boldz\) em qualquer passo \(i\). Isso significa que, na linha de descarte (Linha~\ref{linha}), a política recebe parâmetros \(\bolds_{0:i-1}\) e \(\boldz\), conferindo-lhe um poder preditivo. Para problemas intrinsecamente *online*, como o caching, algoritmos offline não são implementáveis na prática. Contudo, são essenciais como base comparativa para os algoritmos online, conforme veremos.

Um algoritmo offline \(\Acal_\pi\) é \textbf{ótimo} se \(\forall z \in \Seq(\Zcal)\), \(c(\pi, z) = \opt(z)\). \textcite{Belady66} propôs um simples algoritmo offline ótimo \(\Acal_{\bold{FIF}}\) que implementa uma política denominada \textit{Furthest-in-the-Future} (abreviada por \textbf{FIF}). Essa política descarta a página do cache que permanecerá sem pedidos pelo maior tempo no futuro. Definemos a política de forma explícica. Primeiramente, para \(z \in \Seq(\Zcal)\), uma entrada qualquer de tamanho \(n\), definimos a função \textbf{próxima ocorrência} \(\tau \colon \Zcal \to \Naturals\) que associa para índice \(i \in [n]\) e página \(x \in \Zcal\) o índice da sua (possível) próxima ocorrência em \(\boldz\). Formalmente,

\begin{equation*}
\tau(i, x) =
\begin{cases}
  \min \{ i' > i \mid z_{i'} = x \}, & \text{se tal } i' \text{ existe}, \\
  +\infty, & \text{caso contrário}.
\end{cases}
\end{equation*}

Assim, a política \( \pi_{\mathbf{FIF}} \) é definida por, \( \pi_{\mathbf{FIF}} (\bolds_{1:i}, \boldz) = \arg {\max_{x \in s_i} \tau(i, x)}\). A prova da otimalidade de \(\Acal_{\pi_{\mathbf{FIF}}}\) não é trivial, e apresentamos uma demonstração no apêndice.

\begin{definition}
Definimos a função \(\theta : [n] \times \Zcal \to \mathbb{N}_0\) que associa a cada posição \(j \in [n]\) e elemento \(x \in \Zcal\) o instante da última ocorrência de \(x\) antes de \(j\), por
\begin{equation}
\theta(j, x) =
\begin{cases}
  \max\{ t < j \mid \sigma_t = x \}, & \text{se tal } t \text{ existe}, \\[6pt]
  0, & \text{caso contrário}.
\end{cases}
\end{equation}
Aqui adotamos a convenção de que \(\rho(j,x)=0\) significa que \(x\) não ocorreu antes de \(j\).
\end{definition}

\subsection{Análise competitiva}

\textcite{Sleator93} propôs uma forma alternativa de análise de algoritmos online, chamada de \emph{análise competitiva}, em que o o desempenho de um algoritmo online é determinado pela razão entre seu custo e o custo do algoritmo offline\footnote{A partir daqui dispensaremos a palavra "ótimo": sempre nos referimos ao algoritmo offline} no pior caso. Determinando a competitividade de diferentes algoritmos, temos uma base para comparar as estratégias online.

\begin{definition}
  \label{def:comp}
  Seja \(\pi \in \Pi\) uma política de caching e considere o algoritmo \emph{online} de caching associado a ela \(\Acal_\pi\). Dizemos que \(\Acal_\pi\) é \(c\)\textbf{-competitivo} se existe uma constante \(\alpha\) tal que para todo \(z \in \Seq(\Zcal)\),
\begin{equation}
  c(\pi, z ) \le c \opt(z) + \alpha.
\end{equation}
\end{definition}

Quando a \textbf{constante aditiva} \(\alpha\) é menor ou igual a zero (i.e., \(c(\pi, z) \le c \opt(z)\)) podemos dar ênfase e dizer que \(\Acal_\pi\) é \textbf{estritamente \(c\)-competitivo}. Permitir \(\alpha > 0\) reflete o fato que para problemas intrinsecamente online como o problema de caching, existe uma entrada \(z\) arbitrariamente longa que faz \(c(\pi, z)\) ser ilimitado. A constante \(\alpha\) torna-se insignificante a medida que consideramos entradas mais longas. Além disso, mesmo para entradas finitas, o uso de \(\alpha\) permite definir uma razão de desempenho que não depende das condições iniciais, neste caso, o cacho inicial. 

Para cada entrada \(\boldz\), um algoritmo \(c\)-competitivo é garantido de ter um custo dentro do fator \(c\) comparado com o custo mínimo \(\opt(z)\) (a menos da constante aditiva \(\alpha\)). A razão de competitivade é sempre maior ou igual a \(1\) e quanto mais próxima de 1, melhor o algoritmo desempenha comparado com o algortimo offline ótimo que atinge sempre custo \(\opt(z)\).

Se \(\Acal_\pi\) é \(c\)-competitivo, dizemos que o algoritmo \emph{atinge} uma \textbf{razão de competitivade} \(c\). Um algoritmo é chamado \textbf{competitivo} se ele atinge uma razão competitiva \(c\) "constante", ou seja, \(c\) não depende da entrada \(\boldz\) mas pode depender do parâmetro do problema (em geral, o tamanho do cache \(k\)). O ínfimo sobre o conjunto de todos os valores de \(c\) tal que \(\Acal_\pi\) é competitivo é chamado de \textbf{a razão competitiva} de \(\Acal_\pi\) denotada por \(\Rcal(\Acal_\pi)\).

\section{Algortimos determinísticos}

O objetivo geral do problema de caching é encontrar políticas que resultem em um custo baixo para qualquer entrada. A redução das falhas de cache depende, em grande parte, de manter no cache as páginas que serão requisitadas em breve. Com isso, são formuladas políticas que procuram atingir esse objetivo. Seja \(\Acal_\pi\) um algoritmo online para o problema de caching. Dizemos que \(\Acal_\pi\) é \textbf{determinístico} se sua política \(\pi\) é determinística, ou seja, não possui um gerador de números aleatórios para auxiliar suas decisões. Por outro lado, um algoritmo \textbf{aleatorizado} pode tomar decisões aleatórias. Conforme veremos adiante, os algoritmos determinísticos possum menos "poder" e os algoritmos aleatorizados conseguem atingir um desemepenho melhor na teoria. 

\subsection{Políticas clássicas}

Visando incorrer um custo baixo, algumas políticas clássicas são formuladas e os algoritmos correspondentes.  

A política \(\pi_{\mathbf{LRU}}\) (\textit{Least Recently Used}), descarta a página no cache cujo tempo de chegada é o menor, um indicativo que não aparecerá tão breve, pois há muito tempo não foi requisitada. Formalmente, \( \pi_{\mathbf{LRU}} (\bolds_{1:i}, \boldz) = \arg {\max_{x \in s_i} \theta(i, x)}\)

No algortimo \(\mathbf{A_{FIFO}}\) (\textit{First-in, First-out}), \(e(i)\) é o elemento que está há mais tempo no cache. 

No algortimo \(\mathbf{A_{LFU}}\) (\textit{Least Frequently Used}) \(e(i)\) é o elemento no cache que foi menos requisitada no passado.

Aqui, não analisamos os detalhes das implementações e as restrições de tempo e espaço desses algoritmos: nosso foco é o custo do algoritmo, isto é, a quantidade total de falhas de cache, que independente da exata implementação de cada estratégia nos algoritmos.

Foi dito que existe um algoritmo determinístico ótimo offline (que implementa a política \textbf{FIF}) que incorre o custo mínimo para uma entrada \(z\). A dúvida natural que surje é quão próximo do custo mínimo um algoritmo online pode alcançar. Veremos nas próximas seções que nenhum algoritmo online pode igualar esse custo para toda entrada, como é esperado. De fato, somente um algoritmo online que pudesse advinhar os pedidos futuros poderia escolher, em cada passo, o exato elemento \(e(i)\) que minimiza globalmente o custo para uma entrada. Por isso, imaginamos \(\opt(z)\) como um custo intrínseco da entrada. Assim, há uma limitação para os algoritmos online, tanto determinísticos quanto aleatorizados, e veremos em que medida são nas seções seguintes atráves de suas competitividades.

Primeiramente, para algoritmos determinísticos, provamos uma cota cota inferior para a competitividade, mostrando o quão próximos podem ser do algoritmo offline. 

Seja \(z\) uma sequência de elementos de tamanho \(n\). Definimos duas funções auxliares \(\tau(i,x)\) e \(\theta(i,x)\) que dizem o tempo da próxima ocorrência e da última ocorrência do elemento \(x\) na sequência \(z\), respectivamente.

\begin{theorem}
\label{teo:cota_det}
Seja \(\Acal\) um algoritmo determinístico. Então, \(C_{\Acal} \geqslant k\).
\end{theorem}

\begin{proof}

  Pela \Cref{def:comp} devemos provar que \(\Acal\) é no mínimo \(k\)-competitivo. Para isso, basta provar que existe uma entrada \(z\) arbitrariamente grande tal que \(\frac{\Acal(z)} {\opt(z)}\) é no mínimo \(k\). 

  Como \(\Acal\) é determinístico, podemos construir indutivamente uma sequência patológica \(z\) que faz \(\Acal\) falhar em todos passos. Construímos \(z\) assim: tome \(z_1\) igual a qualquer elemento em \(\Zcal \setminus C[0]\). Como \(z_1 \notin C[0]\), o algoritmo falha no primeiro passo. Em seguida, tome \(z_2 = e_1\), e no segundo passo também falha. Tome \(z_3 = e_2\) e assim por diante até \(z_n = e_{n-1}\). Fica claro que \(\Acal\) falha em todos os passos e \(\Acal(z) = n\). Qual o custo \(\opt(z)\)?

  Sabemos que o custo do algoritmo \textbf{FIF} iguala \(\opt(z)\), então calculemos o custo desse algoritmo quando processa \(z\). Pela descrição acima, \(z\) claramente envolve apenas \(|C[0] \cup \{z_1\}| = k+1\) elementos distintos, como o cache tem tamanho \(k\) qualquer algortimo falha em algum passo \(j\). Uma vez que \(e_j\) foi escolhido por \textbf{FIF}, segue que todo elemento \(x \in C_{j-1} \setminus \{e_j\}\) tem \(\tau(j,x) < \tau(j, e_j)\), como há \(k-1\) elementos desses, \(\tau(j, e_j) > j + k-1\). O pedido \(z_{j+1}\) está em \(C_j\) pois o único elemento envolvido fora do cache é \(e_j\) e, portanto, há um acerto. A mesma coisa se segue para os passos \(i = j+1, j+2, \dotsc,\) até \(\tau(j,e_j)\). Assim, o algoritmo acerta \(\tau(j, e_j) - j \ge k-1\) vezes após a falha no passo \(j\). \textbf{Conclusão}: após uma falha de cache, \textbf{FIF} acerta ao menos \(k-1\) vezes para \(z\). Aplicando esse fato para toda a sequência \(z\), chegamos em uma limite inferior para a quantidade de acertos \(n-\opt(z)\),
\begin{equation*}
  n - \opt(z) \ge \opt(z)(k-1).
\end{equation*}
Daí, \(\opt(z) \ge nk = \Acal(z)k\) e \(\frac{\Acal(z)}{\opt(z)} \le k\).

\end{proof}

\subsection{Competitividade das políticas clássicas}

Dada a cota inferior \(k\) para a competitividade de algoritmos determinísticos, perguntamos quão próximo desse limite os algoritmos clássicos chegam. Em particular, começamos provando que a política \(\lru\) atinge esse limite, ou seja, \(C_{\lru} = k\).

Primeiramente, definimos o conceito de \emph{fases} de uma sequência qualquer \(z\) de elementos dado um algoritmo (determinístico? )\(\Acal\).

\begin{definition}
  \label{def:fases}
  \textbf{Partição em fases de \(k\) falhas}. Sejam \(\Acal\) um algoritmo, \(z\) uma entrada qualquer para o algoritmo e considere uma partição \(\Pcal(z) = \Pcal_1, \dotsc, \Pcal_m\) qualquer de \(z\) com tamanho \(m\). Dizemos que \(\Pcal(z)\) é uma \emph{partição em fases de \(k\) falhas} para o algoritmo \(\Acal\) se \(\Acal(P_1)) \le k\) e \(\Acal(P_i) = k\) \(\forall i \in [2,m]\) e chamamos as subsequências \(P_i\), \(i \in [m]\) de \emph{fases} de \(z\) no contexto do algoritmo \(\Acal\). Ou seja, \(\Acal\) falha exatamente \(k\) vezes em cada fase, com exeção da primeira fase em que pode falhar \emph{até} \(k\) vezes. Dado \(\Acal\) e \(z\), usamos \(P_{\Acal}(z)\) para denotar qualquer partição em fases de \(k\) falhas.
\end{definition}

Evidentemente, não existe uma única partição em fases de \(k\) falhas dados \(\Acal\) e \(z\), no entanto, qualquer uma dessas servirá para nosso propósitos. Uma maneira direta de construir uma partição desse tipo é: I) observar o algoritmo \(\Acal\) quando processa \(z\) para calcular \(c_{\Acal}(i)\) \(\forall i \in [n]\); II) em seguida, começando do fim de \(z\), começar uma nova fase assim que o custo da fase anterior atingir \(k\). Claramante, essa construção respeita as duas condições da \Cref{def:fases}.

\begin{lemma}
  \label{lem:k-part}

  Denotemos \(l_j\) o passo em que uma fase arbitrária \(P_j\) começa para \(j \in [2,m]\) \footnote{Como estamos interessados no comportamento assintótico para entradas suficientemente grandes, supomos sempre que \(m \gg 2\)}. Assim, \(z(l_j)\) é o primeiro elemento de \(P_j\); \(z(l_{j+1}-1)\) é o último. Seja \(p = z(l_j-1)\) o último elemento da fase anterior \(P_{j-1}\). Se \(\Acal = \lru\), então \(P_j\) contém \(k\) elementos distintos que são diferentes de \(p\).
  \begin{proof} Olhamos para os pedidos imediatamente antes das \(k\) falhas de cache durante \(P_j\), então, há três casos: I) todos os pedidos são distintos e diferentes de \(p\); II) nem todos pedidos são distintos; III) todos os pedidos são distintos mas nem todos diferentes de \(p\)

  I) Nada a provar: o lema segue diretamente nesse caso.

  II) Suponha, então, que \(\Acal\) falha duas vezes quando foi pedido um elemento \(q \in P_j\) nos passos \(i_1\) e \(i_2\) com \(l_j \le i_1 < i_2 \le l_{j+1} - 1\). Segue então, que \(q\) foi descartado por \(\lru\) em algum passo intermediário \(i_1 < i < i_2\). Considere os elementos presentes no cache \(C[i-1]\) imediatamente antes da falha no passo \(i\). Pela estratégia do algoritmo, \(q\) é o elemento com menor última ocorrência, então, os outros \(k-1\) elementos, distintos e diferentes de \(q\), foram pedidos nos passos \(i \in [i_1+1, i-1]\). Os elementos \(z(i_1) = q\) e \(z(i) \neq q\) juntamente com os elementos em \(C[i-1] \setminus \{q\}\) formam um conjunto de \(k+1\) páginas distintas em \(P_j\) e, portanto, sempre conterá ao menos \(k\) elementos diferentes de \(p\).

  III) Suponha, então, que ocorre uma falha em algum tempo \(l_j \le i \le l_{j+1}-1\) com \(z(i) = p\). A fase \(j\) começa com \(p\) no cache, \(z(l_k-1) = p \in C_{k_i}\). De forma análoga ao caso II), invocamos o mesmo argumento e concluímos que \(z(l_j-1), z(l_j), \dotsc, z(i)\) contém \(k\) elementos distintos. Então, já que \(z(l_j-1) = p\), há exatamente \(k\) elementos distintos e diferentes de \(p\) em \(z(l_j), \dotsc, z(i) \subset P_j\).

\end{proof}
\end{lemma}

Finalmente, provamos que a \(\lru\) atinge a melhor competitivade para algoritmos determinísticos dada pelo \Cref{teo:cota_det}.

\begin{theorem}
  A competitividade do algoritmo \(\lru\) é igual ao tamanho do cache. Isto é, \(C_{\lru} = k\).
\end{theorem}

\begin{proof}
      \(\opt\) contém \(p\) no começo da fase \(P(i)\), pois esse foi o último pedido da fase anterior. Como nesse instante, ele não pode conter também os outros ao menos \(k\) elementos distintos em \(P(i)\) ocorrerá ao menos uma falha nessa fase. Como o fase é arbitrária segue que \(\opt\) falha ao menos uma vez em toda fase (e a fase inicial), e, portanto, \(\opt(z) \ge m\).

  Assim, para toda entrada \(z\), podemos limitar o custo total fazendo a soma por fases,

    \begin{align*}
    \lru(z) 
    &= \sum_{i=1}^n c_{\lru}(i) = \sum_{j=1}^m \lru(P(j)) 
    \\
    &= \lru(P(1)) + \sum_{j=2}^m \lru(P(j))
    \\
    &= \lru(P(1)) + (m-1)k \le k + (m-1)k = mk.
    \end{align*}

  Logo, para toda entrada \(z\) de tamanho \(n\), \(\frac{\lru(z)}{\opt(z)} \le k\), e juntamento com o \Cref{teo:cota_det} segue o teorema.
\end{proof}

\subsection{Prova de otimalidade da política \textbf{FIF}}

\subsection{Competitividades de \textbf{LFU} e \textbf{FIFO}}

\section{Algoritmos aleatorizados}

O teorema \ref{teorema1} diz que nenhum algoritmo determinístico tem coeficiente de competitividade menor que \(k\). 

Quão bem se saem algoritmos aleatorizados para o problema de caching?

A presente seção mostra os resultados que respondem a pergunta. Damos continuidade na análise competitiva introduzindo o conceito de adversários e suas relações.

\begin{definition}
  Seja \(\mathbb{A}\) o conjunto de algoritmos determinísticos. Um \textbf{algoritmo aleatorizado} \(A_r\) para o problema de caching é uma variável aleatória sobre \(\mathbb{A}\). 
\end{definition}


\subsection{Adversários}

Um algoritmo aleatorizado \(R\) é munido com um gerador de bits aleatórios que permite o descarte de páginas aleatoriamente após uma falha. Nesse caso, o custo \(Custo(R,\sigma)\) passa a ser uma variável aleatória. Para algoritmos determinísticos, a noção de competitividade é estrita: \(A\) processa online uma sequência \(\sigma\) e seu custo é comparado com \(Custo(\textbf{OPT}, \sigma)\), o custo do algoritmo offline ótimo. O mesmo não acontece para algoritmos aleatorizados. De fato, como veremos em seguida, temos três noções distintas de competitividade, resumidas nos três tipos de \textit{adversários}. 

Na análise competitiva, para medir o desempenho de um algoritmo online \(A\), seja ele determinístico ou aleatorizado, comparamos seu custo com o custo de um algoritmo de referência mais "poderoso" para a mesma sequência (a pior possível para \(A\)). 
A partir de agora, por motivos que ficarão claros, usaremos o termo \textit{adversário} para nos referimos a esse algoritmo de referência. Um adversário agora ganha uma dimensão maior e seu comportamento varia de acordo com o tanto de informação que possui de \(A\). O adversário também vai ser o encarregado de produzir a sequência de entrada \(\sigma\) que será usada para os cálculos de custo. O "objetivo" do adversário então é construir entradas patológicas que, no melhor dos mundos, piore o desemepenho de \(A\) e melhore o seu. Definimos, então, três tipos de adversários, agora verbalmente e mais adiante rigorosamente, baseado nisso:

\begin{definition}
  Um adversário \(Q_s\) é um adversário \textbf{simples} se possui conhecimento do funcionamento de \(A\), mas não tem acesso as suas escolhas aleatórias. Esse é o adversário mais fraco possível. Já que não enxerga as respostas de \(A\), pode muito bem construir sua entrada \(\sigma\) antes de \(A\) processar. 
\end{definition}

Passamos aos outros dois tipos de adversários, chamados de \textbf{adaptativos}. São algoritmos que além de enxergarem a implementação de \(A\), têm acesso as respostas de \(A\), isto é, as páginas escolhidas aleatoriamente para descarte. Um adversário desse tipo escolhe \(\sigma_t\) com base nas respostas (conhecidas por ele) de \(A\) quando esse processa \(\sigma_1, \sigma_2, \dots, \sigma_{t-1}\).

Um adversário adaptativo \(Q\) é uma sequência de funções \(q_t : \{S_t \subset [n] : |S_t| = k\} \to [n] + {\text{PARA}}, \ t = [d_q]\). O adversário adaptativo a cada tempo responde a um conjunto de cache $S_t$ e retorna uma página \(\sigma_t\), ou seja, \(\sigma_t = q_t(\sigma_{1:t-1})\). O elemento PARA sinaliza o momento em que o adversário para de gerar pedidos: \(q_t(S_{1:T}) = \text{PARA}\). A sequência de pedidos é formada pelo adversário e é denotada por \(\sigma(A,Q)\). A sequência de conjuntos de caches para a interação de \(A\) com \(Q\) é \(S(A,Q) = (S_1, S_2, \dots, S_n)\). Para a nossa análise competitiva, o algortimo de \(Q\) é sempre ótimo. Assim, a interação de \(A\) com \(Q\) é a sequência \(\sigma_1, S_1, \sigma_2, S_2, \dots, \sigma_{T-1}, S_{T-1}, \text{PARA}\) e o custo de \(A\) é \(\text{Custo}(A, Q) = \text{Custo}(A, \sigma(A, Q))\).

\begin{definition}
  Um adversário adaptativo \(Q\) é um adversário adaptativo \textbf{offline} se, após gerar \(\sigma\), usa \textbf{OPT} para gerar seu custo. Isto é, Custo\((Q, \sigma) = \text{Custo}(\textbf{OPT}, \sigma)\).
\end{definition}

\begin{definition}
  Um adversário adaptativo \(Q\) é um adversário adaptativo \textbf{online} se, mantém seu próprio cache online. Em outras palavras, constrói \(\sigma\) observando a cada tempo as respostas de \(A\), assim como o adversário offline, mas a cada tempo também dá sua resposta (página a ser descartada). 
\end{definition}

\subsection{Competitividade para algortimos aleatorizados}

Analogamente ao que foi feito para algortimos determinísticos, podemos definir o coeficiente de competitividade para algortimos aleatorizados. A única diferença na definição surge no fato que, agora, o custo \(Custo(R, \sigma)\) é uma variável aleatória.

\begin{definition}
  Dizemos que um algoritmo aleatorizado \(R\) para o problema de caching é \(C\)-competitivo se para toda sequência \(\sigma\) existe uma constante \(b\) tal que
  \begin{equation}
    \mathbb{E}[\text{Custo}(R, \sigma)] - C \times \mathbb{E}[\text{Custo}(Q, \sigma)] \leqslant b
  \end{equation}
  
\end{definition}

O coeficiente de competitividade de \(R\), denotado \(C_R\) é o infímo de \(C\) tal que \(R\) é \(C\)-competitivo.

De acordo com o tipo de \(Q\), temos diferentes formas de calcular \(\text{Custo}(Q, \sigma)\) e, portanto, diferentes coeficientes para o mesmo algortimo \(R\). Isso será indicado por um sobescrito em \(C_R\): se \(Q\) é um adversário simples, denotamos o coeficiente de competitivade de \(R\) por \(C_R^{\text{s}}\), se é um adversário adaptativo offline, por \(C_R^{\text{off}}\) e, finalmente, se é um adversário adaptativo online, por \(C_R^{\text{on}}\). A pergunta natural que surje é qual a relação entre essas quantidades para um mesmo algoritmo \(R\).

\subsection{Relações entre as competitividades}

Claramente, o adversário offline é mais poderoso que o online, pois o primeiro pode "esperar" para dar sua resposta após toda sequência ser gerada. O adversário online é forçado a dar suas respostas a cada tempo. Por último, em termos de poder, vem o adversário simples, que possui menos informação que os dois primeiros a cada tempo. Quanto menos "poder" \(Q\) possui, mais chances \(R\) tem de ter um comportamento melhor comparado ao adversário, evidentemente. Um desempenho melhor se traduz por um coeficiente de competitiva mais próximo de um. Portanto, é de se esperar que:

\begin{equation}
  C_R^{\text{s}} \leqslant C_R^{\text{on}} \leqslant C_R^{\text{off}}
\end{equation}

Agora, se olharmos para a classe de algortimos \(R\), podemos definir \(C^{s}\) como o menor coeficiente de competitividade de qualquer algoritmo \(R\), analogamente, definimos \(C^\text{{on}}\) e \(C^\text{{off}}\). Além disso, definimos \(C^{\text{det}}\), o menor coeficiente de competitividade de qualquer algoritmo determinístico, esse, certamente, tem menores chances contra um adversário ótimo offile, visto que o último tem total conhecimento do comportamento do primeiro. Logo, da equação x, conclui-se:

\begin{equation}
  C_R^{\text{s}} \leqslant C_R^{\text{on}} \leqslant C_R^{\text{off}} \leqslant C^{\text{det}}
\end{equation}

Naturalmente, surge a pergunta sobre como esses coeficientes se relacionam. Essa inverstigação nos dará resultados gerais sobre os tipos de adversários e quão "bom" pode ser um algoritmo aleatorizado enfrentando esses adversários.


\subsection{Cota inferior para um adversário oblivious}

Primeiramente, provamos um teorema importante. Que nos diz sobre o poder de um adversário simples.

\begin{theorem}
  Seja \(R\) um algoritmo aleatorizado para o problema de caching. Então, \(C_R^{\text{s}} \geqslant H_k\), onde \(H_k = \sum_{j = 1}^{k} 1/j\) é o \(k\)-ésimo número harmônico.
  \begin{proof}
  \end{proof}
\end{theorem}


\subsection{Algortimo Marker}

Descrevemos um famoso algoritmo aleatorizado para o problema de caching que atinge competitividade próxima ao limite inferior do teorema x.

Algoritmo Marker: 

\begin{theorem}

  O algoritmo Marker é (\(2H_k\))-competitivo.
  
  \begin{proof}
    
  \end{proof}
  
\end{theorem}


Existe um algoritmo que atinge competitividade \(H_k\) em geral.

\subsection{Notação funcional para adversários}

A seção anterior mostra que existem algoritmos aleatorizados com competitividade simples substancialmente menor do que qualquer algoritmo determinístico, visto que \(H_k = O(log k)\), enquanto, pelo teorma x, algoritmos determinísticos têm competitividade no mínimo \(k\).

Investigamos agora se uma situação semelhante acontece com adversários adaptativos. Ou seja, se é possível construir algoritmos com coeficientes de competitividade baixos. Veremos que não é possível e concluímos então que os adversários adapativos provam ser relativamente poderosos se comparados com adversários simples.

Mas antes, vamos definir formalmente algoritmos aleatorizados com a mesma notação usada para algoritmos determinísticos.
