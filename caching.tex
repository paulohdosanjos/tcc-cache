% LTeX: language=pt-BR
\chapter{Problema de caching tradicional}

O problema de \emph{caching} considera um sistema com dois níveis de memória: uma memória rápida ou cache de tamanho \(k\) e uma memória lenta de tamanho \(m \gg k\). Uma sequência de itens (páginas), chamada de sequência de pedidos, é fornecida como entrada ao \emph{algoritmo de caching} e deve ser servida em ordem. Se a página solicitada estiver no cache, o pedido é satisfeito com custo zero e dizemos que houve um \emph{acerto de cache}. Se a página solicitada não estiver no cache e sim na memória lenta, acontece uma \emph{falha de cache}. Nesse caso, o algoritmo traz a página da memória lenta e coloca no cache antes de satisfazer o pedido. Se o cache estiver cheio, uma página no cache deve ser descartada. O algoritmo, então, escolhe uma página a ser descartada de acordo com uma regra de substituição ou \emph{política}. O objetivo do problema é encontrar uma política que minimize a quantidade de falhas de cache. Antes de entrarmos nos modelos de predições, estudaremos o problema tradicional, ou seja, sem predições. 

\section{Notação}

\bgroup
	\renewcommand{\arraystretch}{1.2}
	\begin{table}[htbp]
	  \caption{Notação básica}
          \label{tbl:notation}
	  \centering
	  \begin{tabular}{r c p{13cm}}
	    \toprule
            \(\ind{P}\) & \(\coloneqq\)& \(1\) se o predicado \(P\) for verdadeiro, e zero caso contrário \\
            \([n]\)
            & \(\coloneqq\)
            & \(\{1, \dotsc, n\}\) para cada \(n
              \in \Naturals\)\\
            \(\binom{X}{n}\) & \(\coloneqq\)& o conjunto de todos os subconjuntos de \(X\) de tamanho \(n\) para cada \(n \in [|X|]\) \\
            \(X + y\) & \(\coloneqq\)& \(X \cup \{y\}\) dado que \(X\subseteq U\) e \(y \in U\) \\
            \(X - y\) & \(\coloneqq\)& \(X \setminus \{y\}\) dado que \(X\subseteq U\) e \(y \in U\)\\
            \bottomrule
	  \end{tabular}
	\end{table}
\egroup


\bgroup
	\renewcommand{\arraystretch}{1.2}
	\begin{table}[htbp]
	  \caption{Notação para sequências}
          \label{tbl:seq_notation}
	  \centering
	  \begin{tabular}{r c p{13cm}}
	    \toprule
            \(X^*\)
            &\(\coloneqq\)
            & \(\bigcup_{n = 0}^\infty X^n\)\\
            \(\emptyseq\)
            & \(\coloneqq\)
            & a sequência vazia\\
            \(\angb{x_1, \dotsc, x_t}\)
            & \(\coloneqq\)
            & a sequência de tamanho \(t\) em que o \(k\)-th
              elemento é \(x_k\) para cada \(k \in [t]\) \\
            \(\angb{x_i, \dotsc, x_j}\)
            & \(\coloneqq\)
            & \(\emptyseq\) se \(i > j\), a sequência em que o \(k\)-th elemento
              é \(x_{i + k -1}\) para cada \(k \in [j - i + 1]\)
              caso contrário\\
            \(\boldx_{i:j}\)
            & \(\coloneqq\)
            & \(\angb{x_i, \dotsc, x_j}\)\\
            \bottomrule
	  \end{tabular}
	\end{table}
\egroup

\section{Preliminares}

O problema \(\textsc{Caching}(n,k)\) denota o problema de caching para um cache de tamanho \(k\) e sequências de pedidos de tamanho \(n\). É dado um universo \(\Zcal\) (\(|\Zcal| \gg k\)) e elementos \(z_i \in \Zcal\) chegam um por vez em passos \(i \in [n]\). O contéudo do cache imediatamente após processar o pedido \(i\) é \(C_i \subseteq \Zcal\). A entrada é \(z = \angb{z_1, \dotsc, z_n}\) e o conteúdo inicial do cache \(C_0\). Como estamos interessados no comportamento assintótico, ou seja, em entradas grandes, omitiremos \(C_0\) da entrada em geral e também supomos que sempre começa cheio (\(|C_0| = k\)), o conteúdo específico não importa. Um algoritmo \(\Acal\) para o problema de caching, então, deve escolher no passo \(i\) um elemento \(e_i\) para descartar do cache e substituir pelo elemento \(z_i\) em uma falha de cache, isto é, quando \(z_i \notin C_{i-1}\). A saída do problema é a quantidade total de falhas de cache. Os conjuntos \(C_i\) ou, alternativamente, os elementos escolhidos para descarte \(e_i\); em um acerto de cache \(e_i = \emptyset\).

\subsection{Discussão sobre o tamanho da entrada}

comportamento assintótico, condições iniciais, regret time analysis, tamanho da entrada. https://www.cs.cmu.edu/~sleator/papers/amortized-efficiency.pdf. Constante aditiva.

A partir deste ponto, trataremos sempre da instância \textsc{Caching(n,k)} a menos que se fale o contrário, sem necessidade de repetir que o cache tem tamanho \(k\) e a entrada z possui tamanho \(n\). Seja \(\Acal\) um algoritmo online para o problema de caching. Inicialmente, consideramos os algoritmos online determinísticos, que não possuem um gerador de números aleatórios para auxiliar suas decisões. Conforme veremos adiante, isso representa menos poder ao algoritmo: os algoritmos aleatorizados conseguem atingir um desemepenho muito melhor na teoria. Assim, um algoritmo online \(\Acal\) para \(\textsc{Caching}(n,k)\) é determinado pela estratégia que decide os elementos \(e_i\) a serem descartados após uma falha de cache. A redução das falhas de cache depende, em grande parte, de manter no cache as páginas que serão requisitadas em breve. Como o algoritmo é online, deve tomar suas decisões sem conhecimento dos pedidos futuros.

\begin{definition}
  O custo \(\Acal(z)\) quando \(\Acal\) processa a entrada \(z\) é a quantidade total de falhas de cache,
  \begin{equation}
    \Acal(z) = \sum_{i=1}^{|n|} \ind{z_i \notin C_{i-1}}.
  \end{equation}
\end{definition}

O esqueleto de um algoritmo para o problema de caching é, portanto,

\begin{algorithm}
  \caption{Esqueleto de \(\mathbf{A}(C_0, z)\)}
  \label{algo:LRU}
  \begin{algorithmic}
    \Require\\
    \begin{enumerate}[label=(\roman*),ref=\roman*]
    \item Sequência \(z\) de tamanaho \(n\) 
    \item Conteúdo do cache inicial \(C_0\)
    \end{enumerate}

    %
    \Ensure Número total de falhas de cache \(\mathbf{A}(z)\).
    % 
    \State \(c \gets 0\)
    \For{\(i = 1\)~to~\(n\)}
    %
    \If {\(z_i \notin C_{i-1}\) }
      \State escolhe \(e_i \in C_{i-1}\) para descartar
      \State \(C_i \gets C_{i-1} \setminus \{e_i\}\cup \{z_i\} \)
      \State \(c \gets c + 1\)
    \Else
      \State \(C_i \gets C_{i-1}\)
    \EndIf
    \EndFor
    \State \Return \(c\)
  \end{algorithmic}
\end{algorithm}

A escolha de \(e_i\) pode ser feita de forma determística ou não. 

\begin{definition}
  Seja \(\mathbb{A}\) o conjunto de algoritmos determinísticos. Um \textbf{algoritmo aleatorizado} \(A_r\) para o problema de caching é uma variável aleatória sobre \(\mathbb{A}\). 
\end{definition}

\subsection{Algoritmo offline ótimo}

\begin{definition}
  Seja \(\Acal\) o conjunto de algoritmos determinísticos para o problema de caching. Denotamos por \(\opt(z)\) o custo mínimo para uma sequência de páginas \(z\),
  \vredok{Equações tem que ser pontuadas como se fossem a continuação da frase anterior.}
  \begin{equation}
    \opt(z) = \min_{\Acal \in \mathbb{A}} \Acal(z).
  \end{equation}
\end{definition}

\textcite{Belady66} propôs um simples algoritmo offline, isto é, que tem acesso a toda entrada \(z\) a priori, ótimo para o problema de caching. A estratégia do algoritmo de Belady, chamada \textit{Furthest-in-the-Future} (abreviada por \textbf{FIF}), escolhe o elemento \(e_i\) que permanecerá sem pedidos pelo maior tempo no futuro. Ele demonstrou que tal estratégia tem custo \(\opt(z)\) para qualquer entrada \(z\). A prova de otimalidade desse algoritmo guloso não é trivial, então daremos somente o esboço da prova mais adiante no final da seção. Evidentemente, não é possível implementar explicitamente o algoritmo em um cenário online, mesmo assim, seu custo servirá como base de comparação para algorimos online, que é o cerne da \textbf{análise competitiva}.

\subsection{C-competitividade}

\begin{definition}
  \label{def:comp}
  Para \(\alpha > 0\), dizemos que um algoritmo \(\Acal\) para o problema de caching é \(\alpha\)\textbf{-competitivo} se
\begin{equation}
  \mathbb{E}[\Acal(z)] \le \alpha \opt(z), \quad \forall z \in [\Zcal]^n.
\end{equation}

Evidentemente, para algoritmos determinísticos, a esperança pode ser dispensada. Além disso, o \textbf{coeficiente de competitividade}, ou simplesmente competitividade, de \(\Acal\), denotado \(C_{\Acal}\), é o ínfimo de \(\alpha\) tal que \(\Acal\) é \(\alpha\)-competitivo,
\end{definition}

Caracterizamos assim a análise competitiva, em que o o desemepenho de um algoritmo online é determinado pela razão entre seu custo e o custo do algoritmo offline\footnote{A partir daqui dispensaremos a palavra "ótimo": sempre que aparar algoritmo offline será o ótimo} no pior caso. Determinando a competitividade de diferentes algoritmos, temos uma base para comparar as estratégias online. 

\subsection{Algortimos determinísticos clássicos}

Visanto incorrer um custo baixo, algumas estratégias clássicas são formuladas e os algoritmos correspondentes.  

No algortimo \(\lru\) (\textit{Least Recently Used}), \(e_i\) é o elemento no cache cujo último pedido é mais antigo como um indicativo que não aparecerá tão breve, pois há muito tempo não foi requisitada.

No algortimo \(\mathbf{A_{FIFO}}\) (\textit{First-in, First-out}), \(e_i\) é o elemento que está há mais tempo no cache. 

No algortimo \(\mathbf{A_{LFU}}\) (\textit{Least Frequently Used}) \(e_i\) é o elemento no cache que foi menos requisitada no passado.

Aqui, não analisamos os detalhes das implementações e as restrições de tempo e espaço desses algoritmos: nosso foco é o custo do algoritmo, isto é, a quantidade total de falhas de cache, que independente da exata implementação de cada estratégia nos algoritmos.

Foi dito que existe um algoritmo determinístico ótimo offline (que implementa a política \textbf{FIF}) que incorre o custo mínimo para uma entrada \(z\). A dúvida natural que surje é quão próximo do custo mínimo um algoritmo online pode alcançar. Veremos nas próximas seções que nenhum algoritmo online pode igualar esse custo para toda entrada, como é esperado. De fato, somente um algoritmo online que pudesse advinhar os pedidos futuros poderia escolher, em cada passo, o exato elemento \(e_i\) que minimiza globalmente o custo para uma entrada. Por isso, imaginamos \(\opt(z)\) como um custo intrínseco da entrada. Assim, há uma limitação para os algoritmos online, tanto determinísticos quanto aleatorizados, e veremos em que medida são nas seções seguintes atráves de suas competitividades.

Primeiramente, para algoritmos determinísticos, provamos uma cota cota inferior para a competitividade, mostrando o quão próximos podem ser do algoritmo offline. 

Seja \(z\) uma sequência de elementos de tamanho \(n\). Definimos duas funções auxliares \(\tau(i,x)\) e \(\theta(i,x)\) que dizem o tempo da próxima ocorrência e da última ocorrência do elemento \(x\) na sequência \(z\), respectivamente.

\begin{definition}
 Definimos a função \(\tau : [n] \times \Zcal \to \mathbb{N} \cup \{\infty\}\), que associa a cada posição \(j \in [n]\) e elemento \(x \in \Zcal\) o passo da próxima ocorrência de \(x\) após \(j\), como
\begin{equation}
\tau(i, x) =
\begin{cases}
  \min \{ i' > i \mid z_{i'} = x \}, & \text{se tal } i \text{ existe}, \\
  +\infty, & \text{caso contrário}.
\end{cases}
\end{equation}
\end{definition}

\begin{definition}
Definimos a função \(\theta : [n] \times \Zcal \to \mathbb{N}_0\) que associa a cada posição \(j \in [n]\) e elemento \(x \in \Zcal\) o instante da última ocorrência de \(x\) antes de \(j\), por
\begin{equation}
\\theta(j, x) =
\begin{cases}
  \max\{ t < j \mid \sigma_t = x \}, & \text{se tal } t \text{ existe}, \\[6pt]
  0, & \text{caso contrário}.
\end{cases}
\end{equation}
Aqui adotamos a convenção de que \(\rho(j,x)=0\) significa que \(x\) não ocorreu antes de \(j\).
\end{definition}

\begin{theorem}
\label{teo:cota_det}
Seja \(\Acal\) um algoritmo determinístico. Então, \(C_{\Acal} \geqslant k\).
\end{theorem}

\begin{proof}

  Pela \Cref{def:comp} devemos provar que \(\Acal\) é no mínimo \(k\)-competitivo. Para isso, basta provar que existe uma entrada \(z\) arbitrariamente grande tal que \(\frac{\Acal(z)} {\opt(z)}\) é no mínimo \(k\). 

  Como \(\Acal\) é determinístico, podemos construir indutivamente uma sequência patológica \(z\) que faz \(\Acal\) falhar em todos passos. Construímos \(z\) assim: tome \(z_1\) igual a qualquer elemento em \(\Zcal \setminus C_0\). Como \(z_1 \notin C_0\), o algoritmo falha no primeiro passo. Em seguida, tome \(z_2 = e_1\), e no segundo passo também falha. Tome \(z_3 = e_2\) e assim por diante até \(z_n = e_{n-1}\). Fica claro que \(\Acal\) falha em todos os passos e \(\Acal(z) = n\). Qual o custo \(\opt(z)\)?

  Sabemos que o custo do algoritmo \textbf{FIF} iguala \(\opt(z)\), então calculemos o custo desse algoritmo quando processa \(z\). Pela descrição acima, \(z\) claramente envolve apenas \(|C_0 \cup \{z_1\}| = k+1\) elementos distintos, como o cache tem tamanho \(k\) qualquer algortimo falha em algum passo \(j\). Uma vez que \(e_j\) foi escolhido por \textbf{FIF}, segue que todo elemento \(x \in C_{j-1} \setminus \{e_j\}\) tem \(\tau(j,x) < \tau(j, e_j)\), como há \(k-1\) elementos desses, \(\tau(j, e_j) > j + k-1\). O pedido \(z_{j+1}\) está em \(C_j\) pois o único elemento envolvido fora do cache é \(e_j\) e, portanto, há um acerto. A mesma coisa se segue para os passos \(i = j+1, j+2, \dotsc,\) até \(\tau(j,e_j)\). Assim, o algoritmo acerta \(\tau(j, e_j) - j \ge k-1\) vezes após a falha no passo \(j\). \textbf{Conclusão}: após uma falha de cache, \textbf{FIF} acerto ao menos \(k-1\) vezes para \(z\). Aplicando esse fato para toda a sequência \(z\), chegamos em uma limite inferior para a quantidade de acertos \(n-\opt(z)\),
\begin{equation*}
  n - \opt(z) \ge \opt(z)(k-1).
\end{equation*}
Daí, \(\opt(z) \ge nk = \Acal(z)k\) e \(\frac{\Acal(z)}{\opt(z)} \le k\).

\end{proof}

\begin{definition}

  \textbf{Blocos de uma sequência}. Seja \(z\) uma sequência de elementos qualquer. Definimos uma sequência \(b\) onde,
\begin{itemize}
    \item \(b_1\) é o maior prefixo de \(z\) que contém no máximo \(k\) páginas distintas.
    \item \(b_i\) (\(1 < i \le m\)) começa imediatamente após \(b_{i-1}\) e termina no ponto anterior ao pedido que faria o total de páginas distintas no bloco ultrapassar \(k\). 
\end{itemize}

\end{definition}

Lembramos que a política \(\lru\) descarta a página do cache com pedido mais antigo. 

\begin{theorem}
  O coeficiente de competitividade do algoritmo \(\lru\) é igual ao tamanho do cache. Isto é, \(C_{\lru} = k\).
\end{theorem}

\begin{proof}
  Seja \(z\) uma sequência de pedidos qualquer. Considere um tempo o em que ocorreu uma falha qualquer para a página \(p\) no bloco \(b\).
  Nesse ponto, \(p\) é a página mais recentemente solicitada e, para a política \textbf{LRU}, está no fim da fila de prioridade para sair novamente. Observe que ela não voltará ao início da fila enquanto o bloco \(b\) não acabar. De fato, o cache nesse ponto contém \(k-1\) páginas com prioridade de saída maior que \(p\) e, pela definição de bloco, ocorrerão pedidos a no máximo \(k-1\) páginas distintas: \(p\) está garantido de não sair novamente nesse bloco. Como \(p\) é arbitrário, segue que cada página de um bloco pode aumentar em no máximo um o custo total e como há \(k\) páginas por bloco, um bloco pode falhar no máximo \(k\) vezes. Então, \text{LRU} incorre um custo no máximo \(km\), onde \(m\) é a quantidade de blocos em \(\sigma\). Para \textbf{OPT}, o argumento é um pouco diferente. Considere o bloco \(b_1\) mais o primeiro pedido \(p\) de \(b_2\). Esse conjunto contém \(k+1\) páginas distintas, e como o cache tem tamanho \(k\), nenhum algoritmo poderá servir esse conjunto sem falhar ao menos uma vez. Após servir o pedido \(p\), o cache deve conter somente \(k-1\) páginas diferentes de \(p\). Mas, como \(\sigma_2\) é maximal, o restante de \(\sigma_2\) mais o primeiro pedido de \(\sigma_3\) contém pedidos para \(k\) páginas diferentes de \(p\), incorrendo em pelo menos uma falta nesse conjunto. E assim por diante, resultando em pelo menos \(b - 1\) cache misses (se houver um só bloco \textbf{OPT} não falha). Então, 

  \begin{equation*}
    \frac{\text{Custo}(\bold{LRU}, \sigma)}{\text{Custo}(\bold{OPT}, \sigma)} \leqslant kb/(b-1)
  \end{equation*}

  Segue que existem sequências arbitrariamente longas em que \text{LRU} erra no máximo \(k\) vezes menos do que \textbf{OPT}. Pelo Teorema \ref{teorema1}, nenhum algoritmo determinístico pode ter coeficiente maior que \(k\). Logo, \(C_{\bold{LRU}} = k\).
\end{proof}

\subsection{Prova de otimalidade da política \textbf{FIF}}

\subsection{Competitividades de \textbf{LFU} e \textbf{FIFO}}

\section{Algoritmos aleatorizados}

O teorema \ref{teorema1} diz que nenhum algoritmo determinístico tem coeficiente de competitividade menor que \(k\). 

Quão bem se saem algoritmos aleatorizados para o problema de caching?

A presente seção mostra os resultados que respondem a pergunta. Damos continuidade na análise competitiva introduzindo o conceito de adversários e suas relações.

\subsection{Adversários}

Um algoritmo aleatorizado \(R\) é munido com um gerador de bits aleatórios que permite o descarte de páginas aleatoriamente após uma falha. Nesse caso, o custo \(Custo(R,\sigma)\) passa a ser uma variável aleatória. Para algoritmos determinísticos, a noção de competitividade é estrita: \(A\) processa online uma sequência \(\sigma\) e seu custo é comparado com \(Custo(\textbf{OPT}, \sigma)\), o custo do algoritmo offline ótimo. O mesmo não acontece para algoritmos aleatorizados. De fato, como veremos em seguida, temos três noções distintas de competitividade, resumidas nos três tipos de \textit{adversários}. 

Na análise competitiva, para medir o desempenho de um algoritmo online \(A\), seja ele determinístico ou aleatorizado, comparamos seu custo com o custo de um algoritmo de referência mais "poderoso" para a mesma sequência (a pior possível para \(A\)). 
A partir de agora, por motivos que ficarão claros, usaremos o termo \textit{adversário} para nos referimos a esse algoritmo de referência. Um adversário agora ganha uma dimensão maior e seu comportamento varia de acordo com o tanto de informação que possui de \(A\). O adversário também vai ser o encarregado de produzir a sequência de entrada \(\sigma\) que será usada para os cálculos de custo. O "objetivo" do adversário então é construir entradas patológicas que, no melhor dos mundos, piore o desemepenho de \(A\) e melhore o seu. Definimos, então, três tipos de adversários, agora verbalmente e mais adiante rigorosamente, baseado nisso:

\begin{definition}
  Um adversário \(Q_s\) é um adversário \textbf{simples} se possui conhecimento do funcionamento de \(A\), mas não tem acesso as suas escolhas aleatórias. Esse é o adversário mais fraco possível. Já que não enxerga as respostas de \(A\), pode muito bem construir sua entrada \(\sigma\) antes de \(A\) processar. 
\end{definition}

Passamos aos outros dois tipos de adversários, chamados de \textbf{adaptativos}. São algoritmos que além de enxergarem a implementação de \(A\), têm acesso as respostas de \(A\), isto é, as páginas escolhidas aleatoriamente para descarte. Um adversário desse tipo escolhe \(\sigma_t\) com base nas respostas (conhecidas por ele) de \(A\) quando esse processa \(\sigma_1, \sigma_2, \dots, \sigma_{t-1}\).

Um adversário adaptativo \(Q\) é uma sequência de funções \(q_t : \{S_t \subset [n] : |S_t| = k\} \to [n] + {\text{PARA}}, \ t = [d_q]\). O adversário adaptativo a cada tempo responde a um conjunto de cache $S_t$ e retorna uma página \(\sigma_t\), ou seja, \(\sigma_t = q_t(\sigma_{1:t-1})\). O elemento PARA sinaliza o momento em que o adversário para de gerar pedidos: \(q_t(S_{1:T}) = \text{PARA}\). A sequência de pedidos é formada pelo adversário e é denotada por \(\sigma(A,Q)\). A sequência de conjuntos de caches para a interação de \(A\) com \(Q\) é \(S(A,Q) = (S_1, S_2, \dots, S_n)\). Para a nossa análise competitiva, o algortimo de \(Q\) é sempre ótimo. Assim, a interação de \(A\) com \(Q\) é a sequência \(\sigma_1, S_1, \sigma_2, S_2, \dots, \sigma_{T-1}, S_{T-1}, \text{PARA}\) e o custo de \(A\) é \(\text{Custo}(A, Q) = \text{Custo}(A, \sigma(A, Q))\).

\begin{definition}
  Um adversário adaptativo \(Q\) é um adversário adaptativo \textbf{offline} se, após gerar \(\sigma\), usa \textbf{OPT} para gerar seu custo. Isto é, Custo\((Q, \sigma) = \text{Custo}(\textbf{OPT}, \sigma)\).
\end{definition}

\begin{definition}
  Um adversário adaptativo \(Q\) é um adversário adaptativo \textbf{online} se, mantém seu próprio cache online. Em outras palavras, constrói \(\sigma\) observando a cada tempo as respostas de \(A\), assim como o adversário offline, mas a cada tempo também dá sua resposta (página a ser descartada). 
\end{definition}

\subsection{Competitividade para algortimos aleatorizados}

Analogamente ao que foi feito para algortimos determinísticos, podemos definir o coeficiente de competitividade para algortimos aleatorizados. A única diferença na definição surge no fato que, agora, o custo \(Custo(R, \sigma)\) é uma variável aleatória.

\begin{definition}
  Dizemos que um algoritmo aleatorizado \(R\) para o problema de caching é \(C\)-competitivo se para toda sequência \(\sigma\) existe uma constante \(b\) tal que
  \begin{equation}
    \mathbb{E}[\text{Custo}(R, \sigma)] - C \times \mathbb{E}[\text{Custo}(Q, \sigma)] \leqslant b
  \end{equation}
  
\end{definition}

O coeficiente de competitividade de \(R\), denotado \(C_R\) é o infímo de \(C\) tal que \(R\) é \(C\)-competitivo.

De acordo com o tipo de \(Q\), temos diferentes formas de calcular \(\text{Custo}(Q, \sigma)\) e, portanto, diferentes coeficientes para o mesmo algortimo \(R\). Isso será indicado por um sobescrito em \(C_R\): se \(Q\) é um adversário simples, denotamos o coeficiente de competitivade de \(R\) por \(C_R^{\text{s}}\), se é um adversário adaptativo offline, por \(C_R^{\text{off}}\) e, finalmente, se é um adversário adaptativo online, por \(C_R^{\text{on}}\). A pergunta natural que surje é qual a relação entre essas quantidades para um mesmo algoritmo \(R\).

\subsection{Relações entre as competitividades}

Claramente, o adversário offline é mais poderoso que o online, pois o primeiro pode "esperar" para dar sua resposta após toda sequência ser gerada. O adversário online é forçado a dar suas respostas a cada tempo. Por último, em termos de poder, vem o adversário simples, que possui menos informação que os dois primeiros a cada tempo. Quanto menos "poder" \(Q\) possui, mais chances \(R\) tem de ter um comportamento melhor comparado ao adversário, evidentemente. Um desempenho melhor se traduz por um coeficiente de competitiva mais próximo de um. Portanto, é de se esperar que:

\begin{equation}
  C_R^{\text{s}} \leqslant C_R^{\text{on}} \leqslant C_R^{\text{off}}
\end{equation}

Agora, se olharmos para a classe de algortimos \(R\), podemos definir \(C^{s}\) como o menor coeficiente de competitividade de qualquer algoritmo \(R\), analogamente, definimos \(C^\text{{on}}\) e \(C^\text{{off}}\). Além disso, definimos \(C^{\text{det}}\), o menor coeficiente de competitividade de qualquer algoritmo determinístico, esse, certamente, tem menores chances contra um adversário ótimo offile, visto que o último tem total conhecimento do comportamento do primeiro. Logo, da equação x, conclui-se:

\begin{equation}
  C_R^{\text{s}} \leqslant C_R^{\text{on}} \leqslant C_R^{\text{off}} \leqslant C^{\text{det}}
\end{equation}

Naturalmente, surge a pergunta sobre como esses coeficientes se relacionam. Essa inverstigação nos dará resultados gerais sobre os tipos de adversários e quão "bom" pode ser um algoritmo aleatorizado enfrentando esses adversários.


\subsection{Cota inferior para um adversário oblivious}

Primeiramente, provamos um teorema importante. Que nos diz sobre o poder de um adversário simples.

\begin{theorem}
  Seja \(R\) um algoritmo aleatorizado para o problema de caching. Então, \(C_R^{\text{s}} \geqslant H_k\), onde \(H_k = \sum_{j = 1}^{k} 1/j\) é o \(k\)-ésimo número harmônico.
  \begin{proof}
  \end{proof}
\end{theorem}


\subsection{Algortimo Marker}

Descrevemos um famoso algoritmo aleatorizado para o problema de caching que atinge competitividade próxima ao limite inferior do teorema x.

Algoritmo Marker: 

\begin{theorem}

  O algoritmo Marker é (\(2H_k\))-competitivo.
  
  \begin{proof}
    
  \end{proof}
  
\end{theorem}


Existe um algoritmo que atinge competitividade \(H_k\) em geral.

\subsection{Notação funcional para adversários}

A seção anterior mostra que existem algoritmos aleatorizados com competitividade simples substancialmente menor do que qualquer algoritmo determinístico, visto que \(H_k = O(log k)\), enquanto, pelo teorma x, algoritmos determinísticos têm competitividade no mínimo \(k\).

Investigamos agora se uma situação semelhante acontece com adversários adaptativos. Ou seja, se é possível construir algoritmos com coeficientes de competitividade baixos. Veremos que não é possível e concluímos então que os adversários adapativos provam ser relativamente poderosos se comparados com adversários simples.

Mas antes, vamos definir formalmente algoritmos aleatorizados com a mesma notação usada para algoritmos determinísticos.
